{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_breast_cancer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iGyL4lCGGgE3"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a6b3da72c4ae4202b43fa1700e430b5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3bf43c3a5c214bdf84969f34fb51dfdf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_55ca13b581704c55ae74ab65cd5f2ace",
              "IPY_MODEL_48dfe5b7b62b4da19f064b2269ed3236",
              "IPY_MODEL_edd42e6a52d14b56af373649204c702d"
            ]
          }
        },
        "3bf43c3a5c214bdf84969f34fb51dfdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "55ca13b581704c55ae74ab65cd5f2ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_526069adac874796889ca1769c076375",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Done! Use &#x27;show&#x27; commands to display/save.   ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e654832e37574a78961f221869b77a81"
          }
        },
        "48dfe5b7b62b4da19f064b2269ed3236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_053b1b9cf4674a72881d6d24cc977822",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0cf70891244644218e356593b34bb397"
          }
        },
        "edd42e6a52d14b56af373649204c702d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aa2b9045931348f2a00aabcfbfaa7e31",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " [100%]   00:02 -&gt; (00:00 left)",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3fbd8acb49944b8e85e8b98dc29d5fc7"
          }
        },
        "526069adac874796889ca1769c076375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e654832e37574a78961f221869b77a81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "053b1b9cf4674a72881d6d24cc977822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0cf70891244644218e356593b34bb397": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa2b9045931348f2a00aabcfbfaa7e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3fbd8acb49944b8e85e8b98dc29d5fc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9gJ7O03JQeT",
        "outputId": "95ca3835-3378-4a91-9f14-bbf7b96fd150"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive',force_remount=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJUozBCi7LdD"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "yO8Q3QwiznYs",
        "outputId": "ad315c95-8c1c-4bdf-b1ee-d8bd9e7637e1"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/girlscript/Breast%20Cancer%20Data.csv\")\n",
        "train.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302         M  ...                  0.11890          NaN\n",
              "1    842517         M  ...                  0.08902          NaN\n",
              "2  84300903         M  ...                  0.08758          NaN\n",
              "3  84348301         M  ...                  0.17300          NaN\n",
              "4  84358402         M  ...                  0.07678          NaN\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMVp8ZVm5HQn",
        "outputId": "7cd22985-e7a6-414f-a9d4-9cf961ec815d"
      },
      "source": [
        "train.info()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 569 entries, 0 to 568\n",
            "Data columns (total 33 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   id                       569 non-null    int64  \n",
            " 1   diagnosis                569 non-null    object \n",
            " 2   radius_mean              569 non-null    float64\n",
            " 3   texture_mean             569 non-null    float64\n",
            " 4   perimeter_mean           569 non-null    float64\n",
            " 5   area_mean                569 non-null    float64\n",
            " 6   smoothness_mean          569 non-null    float64\n",
            " 7   compactness_mean         569 non-null    float64\n",
            " 8   concavity_mean           569 non-null    float64\n",
            " 9   concave points_mean      569 non-null    float64\n",
            " 10  symmetry_mean            569 non-null    float64\n",
            " 11  fractal_dimension_mean   569 non-null    float64\n",
            " 12  radius_se                569 non-null    float64\n",
            " 13  texture_se               569 non-null    float64\n",
            " 14  perimeter_se             569 non-null    float64\n",
            " 15  area_se                  569 non-null    float64\n",
            " 16  smoothness_se            569 non-null    float64\n",
            " 17  compactness_se           569 non-null    float64\n",
            " 18  concavity_se             569 non-null    float64\n",
            " 19  concave points_se        569 non-null    float64\n",
            " 20  symmetry_se              569 non-null    float64\n",
            " 21  fractal_dimension_se     569 non-null    float64\n",
            " 22  radius_worst             569 non-null    float64\n",
            " 23  texture_worst            569 non-null    float64\n",
            " 24  perimeter_worst          569 non-null    float64\n",
            " 25  area_worst               569 non-null    float64\n",
            " 26  smoothness_worst         569 non-null    float64\n",
            " 27  compactness_worst        569 non-null    float64\n",
            " 28  concavity_worst          569 non-null    float64\n",
            " 29  concave points_worst     569 non-null    float64\n",
            " 30  symmetry_worst           569 non-null    float64\n",
            " 31  fractal_dimension_worst  569 non-null    float64\n",
            " 32  Unnamed: 32              0 non-null      float64\n",
            "dtypes: float64(31), int64(1), object(1)\n",
            "memory usage: 146.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MiaftGU5_Rc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ffc8b5-63de-413e-e505-e79ca6086a4c"
      },
      "source": [
        "!pip install sweetviz"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sweetviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/6f/58c132de8243a16c64b741dfc2aa8b31af66334ae6858d97c41846afe642/sweetviz-2.0.9-py3-none-any.whl (15.1MB)\n",
            "\u001b[K     |████████████████████████████████| 15.1MB 235kB/s \n",
            "\u001b[?25hCollecting tqdm>=4.43.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/3e/2730d0effc282960dbff3cf91599ad0d8f3faedc8e75720fdf224b31ab24/tqdm-4.59.0-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (3.2.2)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (1.1.5)\n",
            "Requirement already satisfied: importlib-resources>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (5.1.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (1.19.5)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (2.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->sweetviz) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->sweetviz) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->sweetviz) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.3->sweetviz) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.2.0->sweetviz) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.11.1->sweetviz) (1.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1.3->sweetviz) (1.15.0)\n",
            "Installing collected packages: tqdm, sweetviz\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed sweetviz-2.0.9 tqdm-4.59.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH34J_AcSZ9V"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0wrY--iz1Hb",
        "outputId": "9d08c834-5bc5-41d1-9916-479311a4f947"
      },
      "source": [
        "len(train['id'].unique())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "569"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Id0Sy68JFT7-",
        "outputId": "a0164553-3867-4bae-eca0-9d3a1c976fc8"
      },
      "source": [
        "train.corr()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.074626</td>\n",
              "      <td>0.099770</td>\n",
              "      <td>0.073159</td>\n",
              "      <td>0.096893</td>\n",
              "      <td>-0.012968</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.050080</td>\n",
              "      <td>0.044158</td>\n",
              "      <td>-0.022114</td>\n",
              "      <td>-0.052511</td>\n",
              "      <td>0.143048</td>\n",
              "      <td>-0.007526</td>\n",
              "      <td>0.137331</td>\n",
              "      <td>0.177742</td>\n",
              "      <td>0.096781</td>\n",
              "      <td>0.033961</td>\n",
              "      <td>0.055239</td>\n",
              "      <td>0.078768</td>\n",
              "      <td>-0.017306</td>\n",
              "      <td>0.025725</td>\n",
              "      <td>0.082405</td>\n",
              "      <td>0.064720</td>\n",
              "      <td>0.079986</td>\n",
              "      <td>0.107187</td>\n",
              "      <td>0.010338</td>\n",
              "      <td>-0.002968</td>\n",
              "      <td>0.023203</td>\n",
              "      <td>0.035174</td>\n",
              "      <td>-0.044224</td>\n",
              "      <td>-0.029866</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius_mean</th>\n",
              "      <td>0.074626</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.323782</td>\n",
              "      <td>0.997855</td>\n",
              "      <td>0.987357</td>\n",
              "      <td>0.170581</td>\n",
              "      <td>0.506124</td>\n",
              "      <td>0.676764</td>\n",
              "      <td>0.822529</td>\n",
              "      <td>0.147741</td>\n",
              "      <td>-0.311631</td>\n",
              "      <td>0.679090</td>\n",
              "      <td>-0.097317</td>\n",
              "      <td>0.674172</td>\n",
              "      <td>0.735864</td>\n",
              "      <td>-0.222600</td>\n",
              "      <td>0.206000</td>\n",
              "      <td>0.194204</td>\n",
              "      <td>0.376169</td>\n",
              "      <td>-0.104321</td>\n",
              "      <td>-0.042641</td>\n",
              "      <td>0.969539</td>\n",
              "      <td>0.297008</td>\n",
              "      <td>0.965137</td>\n",
              "      <td>0.941082</td>\n",
              "      <td>0.119616</td>\n",
              "      <td>0.413463</td>\n",
              "      <td>0.526911</td>\n",
              "      <td>0.744214</td>\n",
              "      <td>0.163953</td>\n",
              "      <td>0.007066</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture_mean</th>\n",
              "      <td>0.099770</td>\n",
              "      <td>0.323782</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.329533</td>\n",
              "      <td>0.321086</td>\n",
              "      <td>-0.023389</td>\n",
              "      <td>0.236702</td>\n",
              "      <td>0.302418</td>\n",
              "      <td>0.293464</td>\n",
              "      <td>0.071401</td>\n",
              "      <td>-0.076437</td>\n",
              "      <td>0.275869</td>\n",
              "      <td>0.386358</td>\n",
              "      <td>0.281673</td>\n",
              "      <td>0.259845</td>\n",
              "      <td>0.006614</td>\n",
              "      <td>0.191975</td>\n",
              "      <td>0.143293</td>\n",
              "      <td>0.163851</td>\n",
              "      <td>0.009127</td>\n",
              "      <td>0.054458</td>\n",
              "      <td>0.352573</td>\n",
              "      <td>0.912045</td>\n",
              "      <td>0.358040</td>\n",
              "      <td>0.343546</td>\n",
              "      <td>0.077503</td>\n",
              "      <td>0.277830</td>\n",
              "      <td>0.301025</td>\n",
              "      <td>0.295316</td>\n",
              "      <td>0.105008</td>\n",
              "      <td>0.119205</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter_mean</th>\n",
              "      <td>0.073159</td>\n",
              "      <td>0.997855</td>\n",
              "      <td>0.329533</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.986507</td>\n",
              "      <td>0.207278</td>\n",
              "      <td>0.556936</td>\n",
              "      <td>0.716136</td>\n",
              "      <td>0.850977</td>\n",
              "      <td>0.183027</td>\n",
              "      <td>-0.261477</td>\n",
              "      <td>0.691765</td>\n",
              "      <td>-0.086761</td>\n",
              "      <td>0.693135</td>\n",
              "      <td>0.744983</td>\n",
              "      <td>-0.202694</td>\n",
              "      <td>0.250744</td>\n",
              "      <td>0.228082</td>\n",
              "      <td>0.407217</td>\n",
              "      <td>-0.081629</td>\n",
              "      <td>-0.005523</td>\n",
              "      <td>0.969476</td>\n",
              "      <td>0.303038</td>\n",
              "      <td>0.970387</td>\n",
              "      <td>0.941550</td>\n",
              "      <td>0.150549</td>\n",
              "      <td>0.455774</td>\n",
              "      <td>0.563879</td>\n",
              "      <td>0.771241</td>\n",
              "      <td>0.189115</td>\n",
              "      <td>0.051019</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area_mean</th>\n",
              "      <td>0.096893</td>\n",
              "      <td>0.987357</td>\n",
              "      <td>0.321086</td>\n",
              "      <td>0.986507</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.177028</td>\n",
              "      <td>0.498502</td>\n",
              "      <td>0.685983</td>\n",
              "      <td>0.823269</td>\n",
              "      <td>0.151293</td>\n",
              "      <td>-0.283110</td>\n",
              "      <td>0.732562</td>\n",
              "      <td>-0.066280</td>\n",
              "      <td>0.726628</td>\n",
              "      <td>0.800086</td>\n",
              "      <td>-0.166777</td>\n",
              "      <td>0.212583</td>\n",
              "      <td>0.207660</td>\n",
              "      <td>0.372320</td>\n",
              "      <td>-0.072497</td>\n",
              "      <td>-0.019887</td>\n",
              "      <td>0.962746</td>\n",
              "      <td>0.287489</td>\n",
              "      <td>0.959120</td>\n",
              "      <td>0.959213</td>\n",
              "      <td>0.123523</td>\n",
              "      <td>0.390410</td>\n",
              "      <td>0.512606</td>\n",
              "      <td>0.722017</td>\n",
              "      <td>0.143570</td>\n",
              "      <td>0.003738</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness_mean</th>\n",
              "      <td>-0.012968</td>\n",
              "      <td>0.170581</td>\n",
              "      <td>-0.023389</td>\n",
              "      <td>0.207278</td>\n",
              "      <td>0.177028</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.659123</td>\n",
              "      <td>0.521984</td>\n",
              "      <td>0.553695</td>\n",
              "      <td>0.557775</td>\n",
              "      <td>0.584792</td>\n",
              "      <td>0.301467</td>\n",
              "      <td>0.068406</td>\n",
              "      <td>0.296092</td>\n",
              "      <td>0.246552</td>\n",
              "      <td>0.332375</td>\n",
              "      <td>0.318943</td>\n",
              "      <td>0.248396</td>\n",
              "      <td>0.380676</td>\n",
              "      <td>0.200774</td>\n",
              "      <td>0.283607</td>\n",
              "      <td>0.213120</td>\n",
              "      <td>0.036072</td>\n",
              "      <td>0.238853</td>\n",
              "      <td>0.206718</td>\n",
              "      <td>0.805324</td>\n",
              "      <td>0.472468</td>\n",
              "      <td>0.434926</td>\n",
              "      <td>0.503053</td>\n",
              "      <td>0.394309</td>\n",
              "      <td>0.499316</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness_mean</th>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.506124</td>\n",
              "      <td>0.236702</td>\n",
              "      <td>0.556936</td>\n",
              "      <td>0.498502</td>\n",
              "      <td>0.659123</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.883121</td>\n",
              "      <td>0.831135</td>\n",
              "      <td>0.602641</td>\n",
              "      <td>0.565369</td>\n",
              "      <td>0.497473</td>\n",
              "      <td>0.046205</td>\n",
              "      <td>0.548905</td>\n",
              "      <td>0.455653</td>\n",
              "      <td>0.135299</td>\n",
              "      <td>0.738722</td>\n",
              "      <td>0.570517</td>\n",
              "      <td>0.642262</td>\n",
              "      <td>0.229977</td>\n",
              "      <td>0.507318</td>\n",
              "      <td>0.535315</td>\n",
              "      <td>0.248133</td>\n",
              "      <td>0.590210</td>\n",
              "      <td>0.509604</td>\n",
              "      <td>0.565541</td>\n",
              "      <td>0.865809</td>\n",
              "      <td>0.816275</td>\n",
              "      <td>0.815573</td>\n",
              "      <td>0.510223</td>\n",
              "      <td>0.687382</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity_mean</th>\n",
              "      <td>0.050080</td>\n",
              "      <td>0.676764</td>\n",
              "      <td>0.302418</td>\n",
              "      <td>0.716136</td>\n",
              "      <td>0.685983</td>\n",
              "      <td>0.521984</td>\n",
              "      <td>0.883121</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.921391</td>\n",
              "      <td>0.500667</td>\n",
              "      <td>0.336783</td>\n",
              "      <td>0.631925</td>\n",
              "      <td>0.076218</td>\n",
              "      <td>0.660391</td>\n",
              "      <td>0.617427</td>\n",
              "      <td>0.098564</td>\n",
              "      <td>0.670279</td>\n",
              "      <td>0.691270</td>\n",
              "      <td>0.683260</td>\n",
              "      <td>0.178009</td>\n",
              "      <td>0.449301</td>\n",
              "      <td>0.688236</td>\n",
              "      <td>0.299879</td>\n",
              "      <td>0.729565</td>\n",
              "      <td>0.675987</td>\n",
              "      <td>0.448822</td>\n",
              "      <td>0.754968</td>\n",
              "      <td>0.884103</td>\n",
              "      <td>0.861323</td>\n",
              "      <td>0.409464</td>\n",
              "      <td>0.514930</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave points_mean</th>\n",
              "      <td>0.044158</td>\n",
              "      <td>0.822529</td>\n",
              "      <td>0.293464</td>\n",
              "      <td>0.850977</td>\n",
              "      <td>0.823269</td>\n",
              "      <td>0.553695</td>\n",
              "      <td>0.831135</td>\n",
              "      <td>0.921391</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.462497</td>\n",
              "      <td>0.166917</td>\n",
              "      <td>0.698050</td>\n",
              "      <td>0.021480</td>\n",
              "      <td>0.710650</td>\n",
              "      <td>0.690299</td>\n",
              "      <td>0.027653</td>\n",
              "      <td>0.490424</td>\n",
              "      <td>0.439167</td>\n",
              "      <td>0.615634</td>\n",
              "      <td>0.095351</td>\n",
              "      <td>0.257584</td>\n",
              "      <td>0.830318</td>\n",
              "      <td>0.292752</td>\n",
              "      <td>0.855923</td>\n",
              "      <td>0.809630</td>\n",
              "      <td>0.452753</td>\n",
              "      <td>0.667454</td>\n",
              "      <td>0.752399</td>\n",
              "      <td>0.910155</td>\n",
              "      <td>0.375744</td>\n",
              "      <td>0.368661</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry_mean</th>\n",
              "      <td>-0.022114</td>\n",
              "      <td>0.147741</td>\n",
              "      <td>0.071401</td>\n",
              "      <td>0.183027</td>\n",
              "      <td>0.151293</td>\n",
              "      <td>0.557775</td>\n",
              "      <td>0.602641</td>\n",
              "      <td>0.500667</td>\n",
              "      <td>0.462497</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.479921</td>\n",
              "      <td>0.303379</td>\n",
              "      <td>0.128053</td>\n",
              "      <td>0.313893</td>\n",
              "      <td>0.223970</td>\n",
              "      <td>0.187321</td>\n",
              "      <td>0.421659</td>\n",
              "      <td>0.342627</td>\n",
              "      <td>0.393298</td>\n",
              "      <td>0.449137</td>\n",
              "      <td>0.331786</td>\n",
              "      <td>0.185728</td>\n",
              "      <td>0.090651</td>\n",
              "      <td>0.219169</td>\n",
              "      <td>0.177193</td>\n",
              "      <td>0.426675</td>\n",
              "      <td>0.473200</td>\n",
              "      <td>0.433721</td>\n",
              "      <td>0.430297</td>\n",
              "      <td>0.699826</td>\n",
              "      <td>0.438413</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <td>-0.052511</td>\n",
              "      <td>-0.311631</td>\n",
              "      <td>-0.076437</td>\n",
              "      <td>-0.261477</td>\n",
              "      <td>-0.283110</td>\n",
              "      <td>0.584792</td>\n",
              "      <td>0.565369</td>\n",
              "      <td>0.336783</td>\n",
              "      <td>0.166917</td>\n",
              "      <td>0.479921</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.164174</td>\n",
              "      <td>0.039830</td>\n",
              "      <td>-0.090170</td>\n",
              "      <td>0.401964</td>\n",
              "      <td>0.559837</td>\n",
              "      <td>0.446630</td>\n",
              "      <td>0.341198</td>\n",
              "      <td>0.345007</td>\n",
              "      <td>0.688132</td>\n",
              "      <td>-0.253691</td>\n",
              "      <td>-0.051269</td>\n",
              "      <td>-0.205151</td>\n",
              "      <td>-0.231854</td>\n",
              "      <td>0.504942</td>\n",
              "      <td>0.458798</td>\n",
              "      <td>0.346234</td>\n",
              "      <td>0.175325</td>\n",
              "      <td>0.334019</td>\n",
              "      <td>0.767297</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius_se</th>\n",
              "      <td>0.143048</td>\n",
              "      <td>0.679090</td>\n",
              "      <td>0.275869</td>\n",
              "      <td>0.691765</td>\n",
              "      <td>0.732562</td>\n",
              "      <td>0.301467</td>\n",
              "      <td>0.497473</td>\n",
              "      <td>0.631925</td>\n",
              "      <td>0.698050</td>\n",
              "      <td>0.303379</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.213247</td>\n",
              "      <td>0.972794</td>\n",
              "      <td>0.951830</td>\n",
              "      <td>0.164514</td>\n",
              "      <td>0.356065</td>\n",
              "      <td>0.332358</td>\n",
              "      <td>0.513346</td>\n",
              "      <td>0.240567</td>\n",
              "      <td>0.227754</td>\n",
              "      <td>0.715065</td>\n",
              "      <td>0.194799</td>\n",
              "      <td>0.719684</td>\n",
              "      <td>0.751548</td>\n",
              "      <td>0.141919</td>\n",
              "      <td>0.287103</td>\n",
              "      <td>0.380585</td>\n",
              "      <td>0.531062</td>\n",
              "      <td>0.094543</td>\n",
              "      <td>0.049559</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture_se</th>\n",
              "      <td>-0.007526</td>\n",
              "      <td>-0.097317</td>\n",
              "      <td>0.386358</td>\n",
              "      <td>-0.086761</td>\n",
              "      <td>-0.066280</td>\n",
              "      <td>0.068406</td>\n",
              "      <td>0.046205</td>\n",
              "      <td>0.076218</td>\n",
              "      <td>0.021480</td>\n",
              "      <td>0.128053</td>\n",
              "      <td>0.164174</td>\n",
              "      <td>0.213247</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.223171</td>\n",
              "      <td>0.111567</td>\n",
              "      <td>0.397243</td>\n",
              "      <td>0.231700</td>\n",
              "      <td>0.194998</td>\n",
              "      <td>0.230283</td>\n",
              "      <td>0.411621</td>\n",
              "      <td>0.279723</td>\n",
              "      <td>-0.111690</td>\n",
              "      <td>0.409003</td>\n",
              "      <td>-0.102242</td>\n",
              "      <td>-0.083195</td>\n",
              "      <td>-0.073658</td>\n",
              "      <td>-0.092439</td>\n",
              "      <td>-0.068956</td>\n",
              "      <td>-0.119638</td>\n",
              "      <td>-0.128215</td>\n",
              "      <td>-0.045655</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter_se</th>\n",
              "      <td>0.137331</td>\n",
              "      <td>0.674172</td>\n",
              "      <td>0.281673</td>\n",
              "      <td>0.693135</td>\n",
              "      <td>0.726628</td>\n",
              "      <td>0.296092</td>\n",
              "      <td>0.548905</td>\n",
              "      <td>0.660391</td>\n",
              "      <td>0.710650</td>\n",
              "      <td>0.313893</td>\n",
              "      <td>0.039830</td>\n",
              "      <td>0.972794</td>\n",
              "      <td>0.223171</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.937655</td>\n",
              "      <td>0.151075</td>\n",
              "      <td>0.416322</td>\n",
              "      <td>0.362482</td>\n",
              "      <td>0.556264</td>\n",
              "      <td>0.266487</td>\n",
              "      <td>0.244143</td>\n",
              "      <td>0.697201</td>\n",
              "      <td>0.200371</td>\n",
              "      <td>0.721031</td>\n",
              "      <td>0.730713</td>\n",
              "      <td>0.130054</td>\n",
              "      <td>0.341919</td>\n",
              "      <td>0.418899</td>\n",
              "      <td>0.554897</td>\n",
              "      <td>0.109930</td>\n",
              "      <td>0.085433</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area_se</th>\n",
              "      <td>0.177742</td>\n",
              "      <td>0.735864</td>\n",
              "      <td>0.259845</td>\n",
              "      <td>0.744983</td>\n",
              "      <td>0.800086</td>\n",
              "      <td>0.246552</td>\n",
              "      <td>0.455653</td>\n",
              "      <td>0.617427</td>\n",
              "      <td>0.690299</td>\n",
              "      <td>0.223970</td>\n",
              "      <td>-0.090170</td>\n",
              "      <td>0.951830</td>\n",
              "      <td>0.111567</td>\n",
              "      <td>0.937655</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.075150</td>\n",
              "      <td>0.284840</td>\n",
              "      <td>0.270895</td>\n",
              "      <td>0.415730</td>\n",
              "      <td>0.134109</td>\n",
              "      <td>0.127071</td>\n",
              "      <td>0.757373</td>\n",
              "      <td>0.196497</td>\n",
              "      <td>0.761213</td>\n",
              "      <td>0.811408</td>\n",
              "      <td>0.125389</td>\n",
              "      <td>0.283257</td>\n",
              "      <td>0.385100</td>\n",
              "      <td>0.538166</td>\n",
              "      <td>0.074126</td>\n",
              "      <td>0.017539</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness_se</th>\n",
              "      <td>0.096781</td>\n",
              "      <td>-0.222600</td>\n",
              "      <td>0.006614</td>\n",
              "      <td>-0.202694</td>\n",
              "      <td>-0.166777</td>\n",
              "      <td>0.332375</td>\n",
              "      <td>0.135299</td>\n",
              "      <td>0.098564</td>\n",
              "      <td>0.027653</td>\n",
              "      <td>0.187321</td>\n",
              "      <td>0.401964</td>\n",
              "      <td>0.164514</td>\n",
              "      <td>0.397243</td>\n",
              "      <td>0.151075</td>\n",
              "      <td>0.075150</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.336696</td>\n",
              "      <td>0.268685</td>\n",
              "      <td>0.328429</td>\n",
              "      <td>0.413506</td>\n",
              "      <td>0.427374</td>\n",
              "      <td>-0.230691</td>\n",
              "      <td>-0.074743</td>\n",
              "      <td>-0.217304</td>\n",
              "      <td>-0.182195</td>\n",
              "      <td>0.314457</td>\n",
              "      <td>-0.055558</td>\n",
              "      <td>-0.058298</td>\n",
              "      <td>-0.102007</td>\n",
              "      <td>-0.107342</td>\n",
              "      <td>0.101480</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness_se</th>\n",
              "      <td>0.033961</td>\n",
              "      <td>0.206000</td>\n",
              "      <td>0.191975</td>\n",
              "      <td>0.250744</td>\n",
              "      <td>0.212583</td>\n",
              "      <td>0.318943</td>\n",
              "      <td>0.738722</td>\n",
              "      <td>0.670279</td>\n",
              "      <td>0.490424</td>\n",
              "      <td>0.421659</td>\n",
              "      <td>0.559837</td>\n",
              "      <td>0.356065</td>\n",
              "      <td>0.231700</td>\n",
              "      <td>0.416322</td>\n",
              "      <td>0.284840</td>\n",
              "      <td>0.336696</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.801268</td>\n",
              "      <td>0.744083</td>\n",
              "      <td>0.394713</td>\n",
              "      <td>0.803269</td>\n",
              "      <td>0.204607</td>\n",
              "      <td>0.143003</td>\n",
              "      <td>0.260516</td>\n",
              "      <td>0.199371</td>\n",
              "      <td>0.227394</td>\n",
              "      <td>0.678780</td>\n",
              "      <td>0.639147</td>\n",
              "      <td>0.483208</td>\n",
              "      <td>0.277878</td>\n",
              "      <td>0.590973</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity_se</th>\n",
              "      <td>0.055239</td>\n",
              "      <td>0.194204</td>\n",
              "      <td>0.143293</td>\n",
              "      <td>0.228082</td>\n",
              "      <td>0.207660</td>\n",
              "      <td>0.248396</td>\n",
              "      <td>0.570517</td>\n",
              "      <td>0.691270</td>\n",
              "      <td>0.439167</td>\n",
              "      <td>0.342627</td>\n",
              "      <td>0.446630</td>\n",
              "      <td>0.332358</td>\n",
              "      <td>0.194998</td>\n",
              "      <td>0.362482</td>\n",
              "      <td>0.270895</td>\n",
              "      <td>0.268685</td>\n",
              "      <td>0.801268</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.771804</td>\n",
              "      <td>0.309429</td>\n",
              "      <td>0.727372</td>\n",
              "      <td>0.186904</td>\n",
              "      <td>0.100241</td>\n",
              "      <td>0.226680</td>\n",
              "      <td>0.188353</td>\n",
              "      <td>0.168481</td>\n",
              "      <td>0.484858</td>\n",
              "      <td>0.662564</td>\n",
              "      <td>0.440472</td>\n",
              "      <td>0.197788</td>\n",
              "      <td>0.439329</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave points_se</th>\n",
              "      <td>0.078768</td>\n",
              "      <td>0.376169</td>\n",
              "      <td>0.163851</td>\n",
              "      <td>0.407217</td>\n",
              "      <td>0.372320</td>\n",
              "      <td>0.380676</td>\n",
              "      <td>0.642262</td>\n",
              "      <td>0.683260</td>\n",
              "      <td>0.615634</td>\n",
              "      <td>0.393298</td>\n",
              "      <td>0.341198</td>\n",
              "      <td>0.513346</td>\n",
              "      <td>0.230283</td>\n",
              "      <td>0.556264</td>\n",
              "      <td>0.415730</td>\n",
              "      <td>0.328429</td>\n",
              "      <td>0.744083</td>\n",
              "      <td>0.771804</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.312780</td>\n",
              "      <td>0.611044</td>\n",
              "      <td>0.358127</td>\n",
              "      <td>0.086741</td>\n",
              "      <td>0.394999</td>\n",
              "      <td>0.342271</td>\n",
              "      <td>0.215351</td>\n",
              "      <td>0.452888</td>\n",
              "      <td>0.549592</td>\n",
              "      <td>0.602450</td>\n",
              "      <td>0.143116</td>\n",
              "      <td>0.310655</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry_se</th>\n",
              "      <td>-0.017306</td>\n",
              "      <td>-0.104321</td>\n",
              "      <td>0.009127</td>\n",
              "      <td>-0.081629</td>\n",
              "      <td>-0.072497</td>\n",
              "      <td>0.200774</td>\n",
              "      <td>0.229977</td>\n",
              "      <td>0.178009</td>\n",
              "      <td>0.095351</td>\n",
              "      <td>0.449137</td>\n",
              "      <td>0.345007</td>\n",
              "      <td>0.240567</td>\n",
              "      <td>0.411621</td>\n",
              "      <td>0.266487</td>\n",
              "      <td>0.134109</td>\n",
              "      <td>0.413506</td>\n",
              "      <td>0.394713</td>\n",
              "      <td>0.309429</td>\n",
              "      <td>0.312780</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.369078</td>\n",
              "      <td>-0.128121</td>\n",
              "      <td>-0.077473</td>\n",
              "      <td>-0.103753</td>\n",
              "      <td>-0.110343</td>\n",
              "      <td>-0.012662</td>\n",
              "      <td>0.060255</td>\n",
              "      <td>0.037119</td>\n",
              "      <td>-0.030413</td>\n",
              "      <td>0.389402</td>\n",
              "      <td>0.078079</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <td>0.025725</td>\n",
              "      <td>-0.042641</td>\n",
              "      <td>0.054458</td>\n",
              "      <td>-0.005523</td>\n",
              "      <td>-0.019887</td>\n",
              "      <td>0.283607</td>\n",
              "      <td>0.507318</td>\n",
              "      <td>0.449301</td>\n",
              "      <td>0.257584</td>\n",
              "      <td>0.331786</td>\n",
              "      <td>0.688132</td>\n",
              "      <td>0.227754</td>\n",
              "      <td>0.279723</td>\n",
              "      <td>0.244143</td>\n",
              "      <td>0.127071</td>\n",
              "      <td>0.427374</td>\n",
              "      <td>0.803269</td>\n",
              "      <td>0.727372</td>\n",
              "      <td>0.611044</td>\n",
              "      <td>0.369078</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.037488</td>\n",
              "      <td>-0.003195</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>-0.022736</td>\n",
              "      <td>0.170568</td>\n",
              "      <td>0.390159</td>\n",
              "      <td>0.379975</td>\n",
              "      <td>0.215204</td>\n",
              "      <td>0.111094</td>\n",
              "      <td>0.591328</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radius_worst</th>\n",
              "      <td>0.082405</td>\n",
              "      <td>0.969539</td>\n",
              "      <td>0.352573</td>\n",
              "      <td>0.969476</td>\n",
              "      <td>0.962746</td>\n",
              "      <td>0.213120</td>\n",
              "      <td>0.535315</td>\n",
              "      <td>0.688236</td>\n",
              "      <td>0.830318</td>\n",
              "      <td>0.185728</td>\n",
              "      <td>-0.253691</td>\n",
              "      <td>0.715065</td>\n",
              "      <td>-0.111690</td>\n",
              "      <td>0.697201</td>\n",
              "      <td>0.757373</td>\n",
              "      <td>-0.230691</td>\n",
              "      <td>0.204607</td>\n",
              "      <td>0.186904</td>\n",
              "      <td>0.358127</td>\n",
              "      <td>-0.128121</td>\n",
              "      <td>-0.037488</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.359921</td>\n",
              "      <td>0.993708</td>\n",
              "      <td>0.984015</td>\n",
              "      <td>0.216574</td>\n",
              "      <td>0.475820</td>\n",
              "      <td>0.573975</td>\n",
              "      <td>0.787424</td>\n",
              "      <td>0.243529</td>\n",
              "      <td>0.093492</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texture_worst</th>\n",
              "      <td>0.064720</td>\n",
              "      <td>0.297008</td>\n",
              "      <td>0.912045</td>\n",
              "      <td>0.303038</td>\n",
              "      <td>0.287489</td>\n",
              "      <td>0.036072</td>\n",
              "      <td>0.248133</td>\n",
              "      <td>0.299879</td>\n",
              "      <td>0.292752</td>\n",
              "      <td>0.090651</td>\n",
              "      <td>-0.051269</td>\n",
              "      <td>0.194799</td>\n",
              "      <td>0.409003</td>\n",
              "      <td>0.200371</td>\n",
              "      <td>0.196497</td>\n",
              "      <td>-0.074743</td>\n",
              "      <td>0.143003</td>\n",
              "      <td>0.100241</td>\n",
              "      <td>0.086741</td>\n",
              "      <td>-0.077473</td>\n",
              "      <td>-0.003195</td>\n",
              "      <td>0.359921</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.365098</td>\n",
              "      <td>0.345842</td>\n",
              "      <td>0.225429</td>\n",
              "      <td>0.360832</td>\n",
              "      <td>0.368366</td>\n",
              "      <td>0.359755</td>\n",
              "      <td>0.233027</td>\n",
              "      <td>0.219122</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perimeter_worst</th>\n",
              "      <td>0.079986</td>\n",
              "      <td>0.965137</td>\n",
              "      <td>0.358040</td>\n",
              "      <td>0.970387</td>\n",
              "      <td>0.959120</td>\n",
              "      <td>0.238853</td>\n",
              "      <td>0.590210</td>\n",
              "      <td>0.729565</td>\n",
              "      <td>0.855923</td>\n",
              "      <td>0.219169</td>\n",
              "      <td>-0.205151</td>\n",
              "      <td>0.719684</td>\n",
              "      <td>-0.102242</td>\n",
              "      <td>0.721031</td>\n",
              "      <td>0.761213</td>\n",
              "      <td>-0.217304</td>\n",
              "      <td>0.260516</td>\n",
              "      <td>0.226680</td>\n",
              "      <td>0.394999</td>\n",
              "      <td>-0.103753</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>0.993708</td>\n",
              "      <td>0.365098</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.977578</td>\n",
              "      <td>0.236775</td>\n",
              "      <td>0.529408</td>\n",
              "      <td>0.618344</td>\n",
              "      <td>0.816322</td>\n",
              "      <td>0.269493</td>\n",
              "      <td>0.138957</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>area_worst</th>\n",
              "      <td>0.107187</td>\n",
              "      <td>0.941082</td>\n",
              "      <td>0.343546</td>\n",
              "      <td>0.941550</td>\n",
              "      <td>0.959213</td>\n",
              "      <td>0.206718</td>\n",
              "      <td>0.509604</td>\n",
              "      <td>0.675987</td>\n",
              "      <td>0.809630</td>\n",
              "      <td>0.177193</td>\n",
              "      <td>-0.231854</td>\n",
              "      <td>0.751548</td>\n",
              "      <td>-0.083195</td>\n",
              "      <td>0.730713</td>\n",
              "      <td>0.811408</td>\n",
              "      <td>-0.182195</td>\n",
              "      <td>0.199371</td>\n",
              "      <td>0.188353</td>\n",
              "      <td>0.342271</td>\n",
              "      <td>-0.110343</td>\n",
              "      <td>-0.022736</td>\n",
              "      <td>0.984015</td>\n",
              "      <td>0.345842</td>\n",
              "      <td>0.977578</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.209145</td>\n",
              "      <td>0.438296</td>\n",
              "      <td>0.543331</td>\n",
              "      <td>0.747419</td>\n",
              "      <td>0.209146</td>\n",
              "      <td>0.079647</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>smoothness_worst</th>\n",
              "      <td>0.010338</td>\n",
              "      <td>0.119616</td>\n",
              "      <td>0.077503</td>\n",
              "      <td>0.150549</td>\n",
              "      <td>0.123523</td>\n",
              "      <td>0.805324</td>\n",
              "      <td>0.565541</td>\n",
              "      <td>0.448822</td>\n",
              "      <td>0.452753</td>\n",
              "      <td>0.426675</td>\n",
              "      <td>0.504942</td>\n",
              "      <td>0.141919</td>\n",
              "      <td>-0.073658</td>\n",
              "      <td>0.130054</td>\n",
              "      <td>0.125389</td>\n",
              "      <td>0.314457</td>\n",
              "      <td>0.227394</td>\n",
              "      <td>0.168481</td>\n",
              "      <td>0.215351</td>\n",
              "      <td>-0.012662</td>\n",
              "      <td>0.170568</td>\n",
              "      <td>0.216574</td>\n",
              "      <td>0.225429</td>\n",
              "      <td>0.236775</td>\n",
              "      <td>0.209145</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.568187</td>\n",
              "      <td>0.518523</td>\n",
              "      <td>0.547691</td>\n",
              "      <td>0.493838</td>\n",
              "      <td>0.617624</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>compactness_worst</th>\n",
              "      <td>-0.002968</td>\n",
              "      <td>0.413463</td>\n",
              "      <td>0.277830</td>\n",
              "      <td>0.455774</td>\n",
              "      <td>0.390410</td>\n",
              "      <td>0.472468</td>\n",
              "      <td>0.865809</td>\n",
              "      <td>0.754968</td>\n",
              "      <td>0.667454</td>\n",
              "      <td>0.473200</td>\n",
              "      <td>0.458798</td>\n",
              "      <td>0.287103</td>\n",
              "      <td>-0.092439</td>\n",
              "      <td>0.341919</td>\n",
              "      <td>0.283257</td>\n",
              "      <td>-0.055558</td>\n",
              "      <td>0.678780</td>\n",
              "      <td>0.484858</td>\n",
              "      <td>0.452888</td>\n",
              "      <td>0.060255</td>\n",
              "      <td>0.390159</td>\n",
              "      <td>0.475820</td>\n",
              "      <td>0.360832</td>\n",
              "      <td>0.529408</td>\n",
              "      <td>0.438296</td>\n",
              "      <td>0.568187</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.892261</td>\n",
              "      <td>0.801080</td>\n",
              "      <td>0.614441</td>\n",
              "      <td>0.810455</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concavity_worst</th>\n",
              "      <td>0.023203</td>\n",
              "      <td>0.526911</td>\n",
              "      <td>0.301025</td>\n",
              "      <td>0.563879</td>\n",
              "      <td>0.512606</td>\n",
              "      <td>0.434926</td>\n",
              "      <td>0.816275</td>\n",
              "      <td>0.884103</td>\n",
              "      <td>0.752399</td>\n",
              "      <td>0.433721</td>\n",
              "      <td>0.346234</td>\n",
              "      <td>0.380585</td>\n",
              "      <td>-0.068956</td>\n",
              "      <td>0.418899</td>\n",
              "      <td>0.385100</td>\n",
              "      <td>-0.058298</td>\n",
              "      <td>0.639147</td>\n",
              "      <td>0.662564</td>\n",
              "      <td>0.549592</td>\n",
              "      <td>0.037119</td>\n",
              "      <td>0.379975</td>\n",
              "      <td>0.573975</td>\n",
              "      <td>0.368366</td>\n",
              "      <td>0.618344</td>\n",
              "      <td>0.543331</td>\n",
              "      <td>0.518523</td>\n",
              "      <td>0.892261</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.855434</td>\n",
              "      <td>0.532520</td>\n",
              "      <td>0.686511</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>concave points_worst</th>\n",
              "      <td>0.035174</td>\n",
              "      <td>0.744214</td>\n",
              "      <td>0.295316</td>\n",
              "      <td>0.771241</td>\n",
              "      <td>0.722017</td>\n",
              "      <td>0.503053</td>\n",
              "      <td>0.815573</td>\n",
              "      <td>0.861323</td>\n",
              "      <td>0.910155</td>\n",
              "      <td>0.430297</td>\n",
              "      <td>0.175325</td>\n",
              "      <td>0.531062</td>\n",
              "      <td>-0.119638</td>\n",
              "      <td>0.554897</td>\n",
              "      <td>0.538166</td>\n",
              "      <td>-0.102007</td>\n",
              "      <td>0.483208</td>\n",
              "      <td>0.440472</td>\n",
              "      <td>0.602450</td>\n",
              "      <td>-0.030413</td>\n",
              "      <td>0.215204</td>\n",
              "      <td>0.787424</td>\n",
              "      <td>0.359755</td>\n",
              "      <td>0.816322</td>\n",
              "      <td>0.747419</td>\n",
              "      <td>0.547691</td>\n",
              "      <td>0.801080</td>\n",
              "      <td>0.855434</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.502528</td>\n",
              "      <td>0.511114</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symmetry_worst</th>\n",
              "      <td>-0.044224</td>\n",
              "      <td>0.163953</td>\n",
              "      <td>0.105008</td>\n",
              "      <td>0.189115</td>\n",
              "      <td>0.143570</td>\n",
              "      <td>0.394309</td>\n",
              "      <td>0.510223</td>\n",
              "      <td>0.409464</td>\n",
              "      <td>0.375744</td>\n",
              "      <td>0.699826</td>\n",
              "      <td>0.334019</td>\n",
              "      <td>0.094543</td>\n",
              "      <td>-0.128215</td>\n",
              "      <td>0.109930</td>\n",
              "      <td>0.074126</td>\n",
              "      <td>-0.107342</td>\n",
              "      <td>0.277878</td>\n",
              "      <td>0.197788</td>\n",
              "      <td>0.143116</td>\n",
              "      <td>0.389402</td>\n",
              "      <td>0.111094</td>\n",
              "      <td>0.243529</td>\n",
              "      <td>0.233027</td>\n",
              "      <td>0.269493</td>\n",
              "      <td>0.209146</td>\n",
              "      <td>0.493838</td>\n",
              "      <td>0.614441</td>\n",
              "      <td>0.532520</td>\n",
              "      <td>0.502528</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.537848</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <td>-0.029866</td>\n",
              "      <td>0.007066</td>\n",
              "      <td>0.119205</td>\n",
              "      <td>0.051019</td>\n",
              "      <td>0.003738</td>\n",
              "      <td>0.499316</td>\n",
              "      <td>0.687382</td>\n",
              "      <td>0.514930</td>\n",
              "      <td>0.368661</td>\n",
              "      <td>0.438413</td>\n",
              "      <td>0.767297</td>\n",
              "      <td>0.049559</td>\n",
              "      <td>-0.045655</td>\n",
              "      <td>0.085433</td>\n",
              "      <td>0.017539</td>\n",
              "      <td>0.101480</td>\n",
              "      <td>0.590973</td>\n",
              "      <td>0.439329</td>\n",
              "      <td>0.310655</td>\n",
              "      <td>0.078079</td>\n",
              "      <td>0.591328</td>\n",
              "      <td>0.093492</td>\n",
              "      <td>0.219122</td>\n",
              "      <td>0.138957</td>\n",
              "      <td>0.079647</td>\n",
              "      <td>0.617624</td>\n",
              "      <td>0.810455</td>\n",
              "      <td>0.686511</td>\n",
              "      <td>0.511114</td>\n",
              "      <td>0.537848</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unnamed: 32</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               id  ...  Unnamed: 32\n",
              "id                       1.000000  ...          NaN\n",
              "radius_mean              0.074626  ...          NaN\n",
              "texture_mean             0.099770  ...          NaN\n",
              "perimeter_mean           0.073159  ...          NaN\n",
              "area_mean                0.096893  ...          NaN\n",
              "smoothness_mean         -0.012968  ...          NaN\n",
              "compactness_mean         0.000096  ...          NaN\n",
              "concavity_mean           0.050080  ...          NaN\n",
              "concave points_mean      0.044158  ...          NaN\n",
              "symmetry_mean           -0.022114  ...          NaN\n",
              "fractal_dimension_mean  -0.052511  ...          NaN\n",
              "radius_se                0.143048  ...          NaN\n",
              "texture_se              -0.007526  ...          NaN\n",
              "perimeter_se             0.137331  ...          NaN\n",
              "area_se                  0.177742  ...          NaN\n",
              "smoothness_se            0.096781  ...          NaN\n",
              "compactness_se           0.033961  ...          NaN\n",
              "concavity_se             0.055239  ...          NaN\n",
              "concave points_se        0.078768  ...          NaN\n",
              "symmetry_se             -0.017306  ...          NaN\n",
              "fractal_dimension_se     0.025725  ...          NaN\n",
              "radius_worst             0.082405  ...          NaN\n",
              "texture_worst            0.064720  ...          NaN\n",
              "perimeter_worst          0.079986  ...          NaN\n",
              "area_worst               0.107187  ...          NaN\n",
              "smoothness_worst         0.010338  ...          NaN\n",
              "compactness_worst       -0.002968  ...          NaN\n",
              "concavity_worst          0.023203  ...          NaN\n",
              "concave points_worst     0.035174  ...          NaN\n",
              "symmetry_worst          -0.044224  ...          NaN\n",
              "fractal_dimension_worst -0.029866  ...          NaN\n",
              "Unnamed: 32                   NaN  ...          NaN\n",
              "\n",
              "[32 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a6b3da72c4ae4202b43fa1700e430b5b",
            "3bf43c3a5c214bdf84969f34fb51dfdf",
            "55ca13b581704c55ae74ab65cd5f2ace",
            "48dfe5b7b62b4da19f064b2269ed3236",
            "edd42e6a52d14b56af373649204c702d",
            "526069adac874796889ca1769c076375",
            "e654832e37574a78961f221869b77a81",
            "053b1b9cf4674a72881d6d24cc977822",
            "0cf70891244644218e356593b34bb397",
            "aa2b9045931348f2a00aabcfbfaa7e31",
            "3fbd8acb49944b8e85e8b98dc29d5fc7"
          ]
        },
        "id": "xdmnUAo_6anw",
        "outputId": "c38157f2-38fb-4ddb-baf4-b19f6b9bf8dc"
      },
      "source": [
        "# using sweetviz\n",
        "import sweetviz as sv\n",
        "report=sv.analyze(train)\n",
        "# Generating report\n",
        "report.show_html('eda_report.html')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6b3da72c4ae4202b43fa1700e430b5b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "                                             |          | [  0%]   00:00 -> (? left)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Report eda_report.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzCGenvYEyQH",
        "outputId": "9c78241e-a7d3-46ad-bd4a-9ab35e2c71f2"
      },
      "source": [
        "report.show_html(filepath='/content/eda_report.html',\n",
        "                 open_browser=True,\n",
        "                 layout='vertical',\n",
        "                 scale=0.7)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Report /content/eda_report.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "owP2Ckkm7IxK",
        "outputId": "714a0f22-9940-46be-c6b7-e2d7579d1222"
      },
      "source": [
        "'''Customize visualization\n",
        "Seaborn and matplotlib visualization.'''\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "plt.style.use('bmh')                    \n",
        "sns.set_style({'axes.grid':False}) \n",
        "\n",
        "'''Plotly visualization .'''\n",
        "import plotly.offline as py\n",
        "from plotly.offline import iplot, init_notebook_mode\n",
        "import plotly.graph_objs as go\n",
        "init_notebook_mode(connected = True) # Required to use plotly offline in jupyter notebook\n",
        "\n",
        "'''Display markdown formatted output like bold, italic bold etc.'''\n",
        "from IPython.display import Markdown\n",
        "def bold(string):\n",
        "    display(Markdown(string))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "HweV-KhI7H4K",
        "outputId": "1585677c-1d5b-4a14-8354-72e8ccb9fd8d"
      },
      "source": [
        "'''Extract numerical variables first.'''\n",
        "num_merged = train.select_dtypes(include = ['int64', 'float64'])\n",
        "#bold('**Numerical variables:**')\n",
        "display(num_merged.head(3))\n",
        "#bold('**Name of numerical variables:**')\n",
        "display(num_merged.columns.values)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.8</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.6</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.9</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.8</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.0</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.5</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  radius_mean  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0    842302        17.99  ...                  0.11890          NaN\n",
              "1    842517        20.57  ...                  0.08902          NaN\n",
              "2  84300903        19.69  ...                  0.08758          NaN\n",
              "\n",
              "[3 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array(['id', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
              "       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
              "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
              "       'radius_se', 'texture_se', 'perimeter_se', 'area_se',\n",
              "       'smoothness_se', 'compactness_se', 'concavity_se',\n",
              "       'concave points_se', 'symmetry_se', 'fractal_dimension_se',\n",
              "       'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst',\n",
              "       'smoothness_worst', 'compactness_worst', 'concavity_worst',\n",
              "       'concave points_worst', 'symmetry_worst',\n",
              "       'fractal_dimension_worst', 'Unnamed: 32'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_GqMsF_STPR"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cH8xU9H-E8u",
        "outputId": "9e92c388-cdea-4ca2-f091-4fe601855532"
      },
      "source": [
        "train['diagnosis'].value_counts()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "B    357\n",
              "M    212\n",
              "Name: diagnosis, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-1DRF3W-SRE"
      },
      "source": [
        "train[\"diagnosis\"].replace([\"B\",\"M\"],[0,1],inplace = True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlUjQmsp9mcr"
      },
      "source": [
        "y = train['diagnosis']\n",
        "X = train.drop(['id','diagnosis','Unnamed: 32'],axis=1)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qE9jVCY8t9m"
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "rs = RobustScaler()\n",
        "X = rs.fit_transform(X)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NLzBLtE_jlS"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL8ss_cWN6Ic"
      },
      "source": [
        "# Deep learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVHRzfZVMFhq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b2c5f8-3087-4e40-8df0-1d6d17505a48"
      },
      "source": [
        "# importing libraries and keras packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Dropout,BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "\n",
        "# Initializing the ann\n",
        "classifier = Sequential()\n",
        "\n",
        "# adding layers to classifier\n",
        "classifier.add(Dense(units = 32 , activation = 'relu',kernel_initializer='he_uniform' , input_dim = X.shape[1]))\n",
        "classifier.add(BatchNormalization())\n",
        "classifier.add(Dropout(0.2))\n",
        "classifier.add(Dense(units = 32 , activation= 'relu',kernel_initializer='he_uniform'))\n",
        "classifier.add(BatchNormalization())\n",
        "classifier.add(Dropout(0.3))\n",
        "classifier.add(Dense(units = 64 , activation= 'relu',kernel_initializer='he_uniform'))\n",
        "classifier.add(BatchNormalization())\n",
        "classifier.add(Dropout(0.4))\n",
        "classifier.add(Dense(units = 1 , activation='sigmoid'))\n",
        "\n",
        "# compiling ann\n",
        "opt = SGD(lr=0.001, momentum=0.9) \n",
        "classifier.compile(optimizer= opt , loss = 'binary_crossentropy' ,metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Fitting the dataset to ann classifier\n",
        "classifier.fit(X_train , y_train,validation_data=(X_test,y_test), batch_size =32 ,epochs = 200)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "12/12 [==============================] - 1s 29ms/step - loss: 0.7968 - accuracy: 0.5672 - val_loss: 0.5849 - val_accuracy: 0.6702\n",
            "Epoch 2/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7103 - accuracy: 0.6096 - val_loss: 0.5209 - val_accuracy: 0.8404\n",
            "Epoch 3/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6984 - accuracy: 0.6196 - val_loss: 0.4609 - val_accuracy: 0.8777\n",
            "Epoch 4/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6274 - accuracy: 0.6889 - val_loss: 0.4129 - val_accuracy: 0.8936\n",
            "Epoch 5/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5145 - accuracy: 0.7628 - val_loss: 0.3771 - val_accuracy: 0.9043\n",
            "Epoch 6/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4090 - accuracy: 0.8054 - val_loss: 0.3452 - val_accuracy: 0.9202\n",
            "Epoch 7/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4255 - accuracy: 0.8132 - val_loss: 0.3176 - val_accuracy: 0.9415\n",
            "Epoch 8/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.3740 - accuracy: 0.8538 - val_loss: 0.2958 - val_accuracy: 0.9468\n",
            "Epoch 9/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3565 - accuracy: 0.8372 - val_loss: 0.2783 - val_accuracy: 0.9468\n",
            "Epoch 10/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3278 - accuracy: 0.8645 - val_loss: 0.2616 - val_accuracy: 0.9468\n",
            "Epoch 11/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.3720 - accuracy: 0.8481 - val_loss: 0.2458 - val_accuracy: 0.9468\n",
            "Epoch 12/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2966 - accuracy: 0.8917 - val_loss: 0.2331 - val_accuracy: 0.9468\n",
            "Epoch 13/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.3179 - accuracy: 0.8910 - val_loss: 0.2233 - val_accuracy: 0.9468\n",
            "Epoch 14/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2445 - accuracy: 0.9076 - val_loss: 0.2142 - val_accuracy: 0.9468\n",
            "Epoch 15/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2612 - accuracy: 0.9057 - val_loss: 0.2043 - val_accuracy: 0.9521\n",
            "Epoch 16/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2527 - accuracy: 0.8990 - val_loss: 0.1954 - val_accuracy: 0.9521\n",
            "Epoch 17/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2201 - accuracy: 0.9015 - val_loss: 0.1881 - val_accuracy: 0.9521\n",
            "Epoch 18/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2663 - accuracy: 0.8743 - val_loss: 0.1820 - val_accuracy: 0.9521\n",
            "Epoch 19/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2627 - accuracy: 0.8713 - val_loss: 0.1756 - val_accuracy: 0.9521\n",
            "Epoch 20/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1905 - accuracy: 0.9387 - val_loss: 0.1714 - val_accuracy: 0.9521\n",
            "Epoch 21/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2471 - accuracy: 0.8841 - val_loss: 0.1662 - val_accuracy: 0.9468\n",
            "Epoch 22/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1999 - accuracy: 0.9338 - val_loss: 0.1615 - val_accuracy: 0.9468\n",
            "Epoch 23/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1875 - accuracy: 0.9414 - val_loss: 0.1577 - val_accuracy: 0.9468\n",
            "Epoch 24/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2202 - accuracy: 0.9129 - val_loss: 0.1543 - val_accuracy: 0.9468\n",
            "Epoch 25/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2192 - accuracy: 0.9048 - val_loss: 0.1507 - val_accuracy: 0.9415\n",
            "Epoch 26/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1977 - accuracy: 0.9174 - val_loss: 0.1486 - val_accuracy: 0.9415\n",
            "Epoch 27/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2029 - accuracy: 0.9380 - val_loss: 0.1458 - val_accuracy: 0.9468\n",
            "Epoch 28/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2405 - accuracy: 0.9030 - val_loss: 0.1430 - val_accuracy: 0.9468\n",
            "Epoch 29/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1998 - accuracy: 0.9319 - val_loss: 0.1405 - val_accuracy: 0.9468\n",
            "Epoch 30/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1880 - accuracy: 0.9375 - val_loss: 0.1378 - val_accuracy: 0.9468\n",
            "Epoch 31/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1437 - accuracy: 0.9485 - val_loss: 0.1357 - val_accuracy: 0.9468\n",
            "Epoch 32/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1371 - accuracy: 0.9647 - val_loss: 0.1332 - val_accuracy: 0.9521\n",
            "Epoch 33/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2431 - accuracy: 0.9123 - val_loss: 0.1320 - val_accuracy: 0.9521\n",
            "Epoch 34/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1851 - accuracy: 0.9190 - val_loss: 0.1303 - val_accuracy: 0.9521\n",
            "Epoch 35/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2186 - accuracy: 0.9232 - val_loss: 0.1283 - val_accuracy: 0.9521\n",
            "Epoch 36/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1870 - accuracy: 0.9247 - val_loss: 0.1262 - val_accuracy: 0.9521\n",
            "Epoch 37/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1292 - accuracy: 0.9536 - val_loss: 0.1250 - val_accuracy: 0.9521\n",
            "Epoch 38/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1882 - accuracy: 0.9238 - val_loss: 0.1233 - val_accuracy: 0.9521\n",
            "Epoch 39/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1995 - accuracy: 0.9215 - val_loss: 0.1222 - val_accuracy: 0.9521\n",
            "Epoch 40/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1635 - accuracy: 0.9409 - val_loss: 0.1210 - val_accuracy: 0.9521\n",
            "Epoch 41/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1774 - accuracy: 0.9116 - val_loss: 0.1198 - val_accuracy: 0.9574\n",
            "Epoch 42/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2119 - accuracy: 0.9355 - val_loss: 0.1184 - val_accuracy: 0.9574\n",
            "Epoch 43/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1591 - accuracy: 0.9570 - val_loss: 0.1166 - val_accuracy: 0.9574\n",
            "Epoch 44/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1701 - accuracy: 0.9226 - val_loss: 0.1151 - val_accuracy: 0.9574\n",
            "Epoch 45/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1653 - accuracy: 0.9364 - val_loss: 0.1145 - val_accuracy: 0.9574\n",
            "Epoch 46/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1515 - accuracy: 0.9482 - val_loss: 0.1133 - val_accuracy: 0.9574\n",
            "Epoch 47/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1044 - accuracy: 0.9667 - val_loss: 0.1126 - val_accuracy: 0.9574\n",
            "Epoch 48/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1554 - accuracy: 0.9535 - val_loss: 0.1114 - val_accuracy: 0.9574\n",
            "Epoch 49/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1789 - accuracy: 0.9218 - val_loss: 0.1100 - val_accuracy: 0.9628\n",
            "Epoch 50/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1814 - accuracy: 0.9171 - val_loss: 0.1095 - val_accuracy: 0.9628\n",
            "Epoch 51/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1521 - accuracy: 0.9478 - val_loss: 0.1091 - val_accuracy: 0.9628\n",
            "Epoch 52/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1389 - accuracy: 0.9529 - val_loss: 0.1087 - val_accuracy: 0.9628\n",
            "Epoch 53/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1660 - accuracy: 0.9212 - val_loss: 0.1079 - val_accuracy: 0.9574\n",
            "Epoch 54/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2056 - accuracy: 0.9015 - val_loss: 0.1067 - val_accuracy: 0.9574\n",
            "Epoch 55/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1336 - accuracy: 0.9523 - val_loss: 0.1061 - val_accuracy: 0.9574\n",
            "Epoch 56/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1416 - accuracy: 0.9554 - val_loss: 0.1058 - val_accuracy: 0.9628\n",
            "Epoch 57/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1803 - accuracy: 0.9281 - val_loss: 0.1055 - val_accuracy: 0.9628\n",
            "Epoch 58/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1558 - accuracy: 0.9553 - val_loss: 0.1050 - val_accuracy: 0.9628\n",
            "Epoch 59/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1126 - accuracy: 0.9722 - val_loss: 0.1047 - val_accuracy: 0.9574\n",
            "Epoch 60/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1434 - accuracy: 0.9527 - val_loss: 0.1046 - val_accuracy: 0.9574\n",
            "Epoch 61/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1908 - accuracy: 0.9222 - val_loss: 0.1039 - val_accuracy: 0.9628\n",
            "Epoch 62/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1350 - accuracy: 0.9384 - val_loss: 0.1033 - val_accuracy: 0.9628\n",
            "Epoch 63/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1106 - accuracy: 0.9605 - val_loss: 0.1027 - val_accuracy: 0.9628\n",
            "Epoch 64/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1228 - accuracy: 0.9504 - val_loss: 0.1030 - val_accuracy: 0.9574\n",
            "Epoch 65/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1484 - accuracy: 0.9419 - val_loss: 0.1027 - val_accuracy: 0.9628\n",
            "Epoch 66/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0988 - accuracy: 0.9731 - val_loss: 0.1021 - val_accuracy: 0.9628\n",
            "Epoch 67/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1342 - accuracy: 0.9401 - val_loss: 0.1014 - val_accuracy: 0.9628\n",
            "Epoch 68/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1063 - accuracy: 0.9640 - val_loss: 0.1006 - val_accuracy: 0.9628\n",
            "Epoch 69/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1464 - accuracy: 0.9363 - val_loss: 0.0992 - val_accuracy: 0.9628\n",
            "Epoch 70/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1219 - accuracy: 0.9651 - val_loss: 0.0981 - val_accuracy: 0.9628\n",
            "Epoch 71/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1265 - accuracy: 0.9682 - val_loss: 0.0978 - val_accuracy: 0.9628\n",
            "Epoch 72/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0930 - accuracy: 0.9653 - val_loss: 0.0978 - val_accuracy: 0.9628\n",
            "Epoch 73/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1502 - accuracy: 0.9421 - val_loss: 0.0969 - val_accuracy: 0.9628\n",
            "Epoch 74/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1527 - accuracy: 0.9359 - val_loss: 0.0962 - val_accuracy: 0.9628\n",
            "Epoch 75/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1261 - accuracy: 0.9516 - val_loss: 0.0958 - val_accuracy: 0.9628\n",
            "Epoch 76/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1439 - accuracy: 0.9438 - val_loss: 0.0955 - val_accuracy: 0.9628\n",
            "Epoch 77/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1107 - accuracy: 0.9727 - val_loss: 0.0954 - val_accuracy: 0.9628\n",
            "Epoch 78/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1161 - accuracy: 0.9572 - val_loss: 0.0948 - val_accuracy: 0.9628\n",
            "Epoch 79/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1238 - accuracy: 0.9584 - val_loss: 0.0946 - val_accuracy: 0.9628\n",
            "Epoch 80/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1509 - accuracy: 0.9260 - val_loss: 0.0942 - val_accuracy: 0.9628\n",
            "Epoch 81/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1303 - accuracy: 0.9571 - val_loss: 0.0937 - val_accuracy: 0.9628\n",
            "Epoch 82/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0931 - accuracy: 0.9607 - val_loss: 0.0938 - val_accuracy: 0.9628\n",
            "Epoch 83/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0934 - accuracy: 0.9738 - val_loss: 0.0928 - val_accuracy: 0.9628\n",
            "Epoch 84/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1000 - accuracy: 0.9687 - val_loss: 0.0922 - val_accuracy: 0.9628\n",
            "Epoch 85/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1524 - accuracy: 0.9414 - val_loss: 0.0914 - val_accuracy: 0.9628\n",
            "Epoch 86/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1463 - accuracy: 0.9564 - val_loss: 0.0916 - val_accuracy: 0.9628\n",
            "Epoch 87/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1314 - accuracy: 0.9448 - val_loss: 0.0913 - val_accuracy: 0.9628\n",
            "Epoch 88/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1120 - accuracy: 0.9651 - val_loss: 0.0914 - val_accuracy: 0.9628\n",
            "Epoch 89/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1139 - accuracy: 0.9561 - val_loss: 0.0908 - val_accuracy: 0.9628\n",
            "Epoch 90/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0809 - accuracy: 0.9675 - val_loss: 0.0905 - val_accuracy: 0.9628\n",
            "Epoch 91/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1200 - accuracy: 0.9602 - val_loss: 0.0896 - val_accuracy: 0.9681\n",
            "Epoch 92/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0702 - accuracy: 0.9851 - val_loss: 0.0900 - val_accuracy: 0.9681\n",
            "Epoch 93/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0984 - accuracy: 0.9686 - val_loss: 0.0898 - val_accuracy: 0.9681\n",
            "Epoch 94/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0836 - accuracy: 0.9735 - val_loss: 0.0893 - val_accuracy: 0.9628\n",
            "Epoch 95/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1522 - accuracy: 0.9420 - val_loss: 0.0892 - val_accuracy: 0.9628\n",
            "Epoch 96/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0878 - accuracy: 0.9735 - val_loss: 0.0887 - val_accuracy: 0.9681\n",
            "Epoch 97/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1317 - accuracy: 0.9445 - val_loss: 0.0883 - val_accuracy: 0.9681\n",
            "Epoch 98/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1210 - accuracy: 0.9562 - val_loss: 0.0882 - val_accuracy: 0.9628\n",
            "Epoch 99/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0804 - accuracy: 0.9742 - val_loss: 0.0882 - val_accuracy: 0.9628\n",
            "Epoch 100/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0889 - accuracy: 0.9612 - val_loss: 0.0877 - val_accuracy: 0.9628\n",
            "Epoch 101/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1060 - accuracy: 0.9648 - val_loss: 0.0879 - val_accuracy: 0.9628\n",
            "Epoch 102/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1012 - accuracy: 0.9565 - val_loss: 0.0879 - val_accuracy: 0.9628\n",
            "Epoch 103/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0672 - accuracy: 0.9813 - val_loss: 0.0873 - val_accuracy: 0.9628\n",
            "Epoch 104/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1675 - accuracy: 0.9419 - val_loss: 0.0864 - val_accuracy: 0.9628\n",
            "Epoch 105/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1707 - accuracy: 0.9349 - val_loss: 0.0857 - val_accuracy: 0.9628\n",
            "Epoch 106/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1250 - accuracy: 0.9544 - val_loss: 0.0859 - val_accuracy: 0.9628\n",
            "Epoch 107/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1638 - accuracy: 0.9185 - val_loss: 0.0856 - val_accuracy: 0.9628\n",
            "Epoch 108/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0949 - accuracy: 0.9697 - val_loss: 0.0855 - val_accuracy: 0.9628\n",
            "Epoch 109/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0735 - accuracy: 0.9808 - val_loss: 0.0850 - val_accuracy: 0.9628\n",
            "Epoch 110/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1027 - accuracy: 0.9515 - val_loss: 0.0847 - val_accuracy: 0.9628\n",
            "Epoch 111/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1050 - accuracy: 0.9608 - val_loss: 0.0840 - val_accuracy: 0.9628\n",
            "Epoch 112/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1067 - accuracy: 0.9729 - val_loss: 0.0838 - val_accuracy: 0.9628\n",
            "Epoch 113/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0840 - accuracy: 0.9787 - val_loss: 0.0835 - val_accuracy: 0.9628\n",
            "Epoch 114/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1476 - accuracy: 0.9398 - val_loss: 0.0834 - val_accuracy: 0.9628\n",
            "Epoch 115/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1529 - accuracy: 0.9524 - val_loss: 0.0833 - val_accuracy: 0.9628\n",
            "Epoch 116/200\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0860 - accuracy: 0.9653 - val_loss: 0.0824 - val_accuracy: 0.9681\n",
            "Epoch 117/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1586 - accuracy: 0.9522 - val_loss: 0.0824 - val_accuracy: 0.9681\n",
            "Epoch 118/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1042 - accuracy: 0.9734 - val_loss: 0.0825 - val_accuracy: 0.9681\n",
            "Epoch 119/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1144 - accuracy: 0.9604 - val_loss: 0.0824 - val_accuracy: 0.9628\n",
            "Epoch 120/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0992 - accuracy: 0.9612 - val_loss: 0.0820 - val_accuracy: 0.9681\n",
            "Epoch 121/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0890 - accuracy: 0.9688 - val_loss: 0.0820 - val_accuracy: 0.9681\n",
            "Epoch 122/200\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0741 - accuracy: 0.9764 - val_loss: 0.0820 - val_accuracy: 0.9681\n",
            "Epoch 123/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0886 - accuracy: 0.9624 - val_loss: 0.0825 - val_accuracy: 0.9628\n",
            "Epoch 124/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1002 - accuracy: 0.9677 - val_loss: 0.0828 - val_accuracy: 0.9628\n",
            "Epoch 125/200\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0691 - accuracy: 0.9878 - val_loss: 0.0826 - val_accuracy: 0.9628\n",
            "Epoch 126/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0670 - accuracy: 0.9794 - val_loss: 0.0826 - val_accuracy: 0.9628\n",
            "Epoch 127/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1501 - accuracy: 0.9388 - val_loss: 0.0824 - val_accuracy: 0.9628\n",
            "Epoch 128/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1198 - accuracy: 0.9480 - val_loss: 0.0820 - val_accuracy: 0.9628\n",
            "Epoch 129/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0763 - accuracy: 0.9712 - val_loss: 0.0814 - val_accuracy: 0.9628\n",
            "Epoch 130/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0770 - accuracy: 0.9843 - val_loss: 0.0818 - val_accuracy: 0.9628\n",
            "Epoch 131/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0755 - accuracy: 0.9680 - val_loss: 0.0816 - val_accuracy: 0.9628\n",
            "Epoch 132/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0895 - accuracy: 0.9679 - val_loss: 0.0820 - val_accuracy: 0.9628\n",
            "Epoch 133/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0781 - accuracy: 0.9733 - val_loss: 0.0822 - val_accuracy: 0.9628\n",
            "Epoch 134/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0825 - accuracy: 0.9712 - val_loss: 0.0827 - val_accuracy: 0.9628\n",
            "Epoch 135/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1080 - accuracy: 0.9490 - val_loss: 0.0825 - val_accuracy: 0.9628\n",
            "Epoch 136/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0609 - accuracy: 0.9720 - val_loss: 0.0821 - val_accuracy: 0.9628\n",
            "Epoch 137/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0835 - accuracy: 0.9775 - val_loss: 0.0818 - val_accuracy: 0.9628\n",
            "Epoch 138/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1180 - accuracy: 0.9601 - val_loss: 0.0814 - val_accuracy: 0.9628\n",
            "Epoch 139/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1235 - accuracy: 0.9605 - val_loss: 0.0820 - val_accuracy: 0.9628\n",
            "Epoch 140/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1256 - accuracy: 0.9648 - val_loss: 0.0824 - val_accuracy: 0.9681\n",
            "Epoch 141/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1400 - accuracy: 0.9302 - val_loss: 0.0824 - val_accuracy: 0.9681\n",
            "Epoch 142/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0618 - accuracy: 0.9713 - val_loss: 0.0824 - val_accuracy: 0.9681\n",
            "Epoch 143/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0829 - accuracy: 0.9772 - val_loss: 0.0820 - val_accuracy: 0.9681\n",
            "Epoch 144/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1067 - accuracy: 0.9574 - val_loss: 0.0822 - val_accuracy: 0.9681\n",
            "Epoch 145/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2791 - accuracy: 0.9081 - val_loss: 0.0818 - val_accuracy: 0.9628\n",
            "Epoch 146/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1251 - accuracy: 0.9365 - val_loss: 0.0816 - val_accuracy: 0.9628\n",
            "Epoch 147/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0695 - accuracy: 0.9788 - val_loss: 0.0812 - val_accuracy: 0.9628\n",
            "Epoch 148/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1193 - accuracy: 0.9537 - val_loss: 0.0809 - val_accuracy: 0.9681\n",
            "Epoch 149/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0861 - accuracy: 0.9585 - val_loss: 0.0806 - val_accuracy: 0.9681\n",
            "Epoch 150/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0741 - accuracy: 0.9744 - val_loss: 0.0805 - val_accuracy: 0.9681\n",
            "Epoch 151/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1029 - accuracy: 0.9691 - val_loss: 0.0807 - val_accuracy: 0.9681\n",
            "Epoch 152/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1081 - accuracy: 0.9652 - val_loss: 0.0810 - val_accuracy: 0.9681\n",
            "Epoch 153/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2057 - accuracy: 0.9215 - val_loss: 0.0807 - val_accuracy: 0.9734\n",
            "Epoch 154/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0546 - accuracy: 0.9890 - val_loss: 0.0805 - val_accuracy: 0.9734\n",
            "Epoch 155/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0675 - accuracy: 0.9867 - val_loss: 0.0805 - val_accuracy: 0.9734\n",
            "Epoch 156/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0695 - accuracy: 0.9687 - val_loss: 0.0801 - val_accuracy: 0.9734\n",
            "Epoch 157/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0922 - accuracy: 0.9604 - val_loss: 0.0801 - val_accuracy: 0.9734\n",
            "Epoch 158/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1516 - accuracy: 0.9302 - val_loss: 0.0800 - val_accuracy: 0.9734\n",
            "Epoch 159/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0766 - accuracy: 0.9684 - val_loss: 0.0798 - val_accuracy: 0.9734\n",
            "Epoch 160/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0669 - accuracy: 0.9738 - val_loss: 0.0796 - val_accuracy: 0.9734\n",
            "Epoch 161/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0993 - accuracy: 0.9693 - val_loss: 0.0798 - val_accuracy: 0.9734\n",
            "Epoch 162/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0759 - accuracy: 0.9738 - val_loss: 0.0798 - val_accuracy: 0.9734\n",
            "Epoch 163/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1102 - accuracy: 0.9584 - val_loss: 0.0799 - val_accuracy: 0.9734\n",
            "Epoch 164/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0655 - accuracy: 0.9824 - val_loss: 0.0803 - val_accuracy: 0.9734\n",
            "Epoch 165/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1272 - accuracy: 0.9529 - val_loss: 0.0805 - val_accuracy: 0.9734\n",
            "Epoch 166/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1203 - accuracy: 0.9624 - val_loss: 0.0804 - val_accuracy: 0.9734\n",
            "Epoch 167/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1460 - accuracy: 0.9535 - val_loss: 0.0803 - val_accuracy: 0.9734\n",
            "Epoch 168/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0753 - accuracy: 0.9585 - val_loss: 0.0803 - val_accuracy: 0.9734\n",
            "Epoch 169/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1046 - accuracy: 0.9663 - val_loss: 0.0801 - val_accuracy: 0.9734\n",
            "Epoch 170/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1053 - accuracy: 0.9783 - val_loss: 0.0802 - val_accuracy: 0.9734\n",
            "Epoch 171/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1108 - accuracy: 0.9624 - val_loss: 0.0804 - val_accuracy: 0.9734\n",
            "Epoch 172/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0958 - accuracy: 0.9600 - val_loss: 0.0805 - val_accuracy: 0.9734\n",
            "Epoch 173/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0855 - accuracy: 0.9661 - val_loss: 0.0808 - val_accuracy: 0.9734\n",
            "Epoch 174/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0967 - accuracy: 0.9603 - val_loss: 0.0808 - val_accuracy: 0.9734\n",
            "Epoch 175/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0737 - accuracy: 0.9752 - val_loss: 0.0811 - val_accuracy: 0.9734\n",
            "Epoch 176/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0665 - accuracy: 0.9837 - val_loss: 0.0813 - val_accuracy: 0.9681\n",
            "Epoch 177/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0497 - accuracy: 0.9883 - val_loss: 0.0811 - val_accuracy: 0.9681\n",
            "Epoch 178/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0994 - accuracy: 0.9699 - val_loss: 0.0813 - val_accuracy: 0.9681\n",
            "Epoch 179/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0726 - accuracy: 0.9736 - val_loss: 0.0810 - val_accuracy: 0.9681\n",
            "Epoch 180/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0623 - accuracy: 0.9846 - val_loss: 0.0805 - val_accuracy: 0.9681\n",
            "Epoch 181/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0785 - accuracy: 0.9733 - val_loss: 0.0800 - val_accuracy: 0.9681\n",
            "Epoch 182/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1009 - accuracy: 0.9650 - val_loss: 0.0799 - val_accuracy: 0.9681\n",
            "Epoch 183/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1075 - accuracy: 0.9739 - val_loss: 0.0798 - val_accuracy: 0.9681\n",
            "Epoch 184/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0769 - accuracy: 0.9785 - val_loss: 0.0794 - val_accuracy: 0.9681\n",
            "Epoch 185/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1114 - accuracy: 0.9555 - val_loss: 0.0790 - val_accuracy: 0.9681\n",
            "Epoch 186/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1201 - accuracy: 0.9540 - val_loss: 0.0789 - val_accuracy: 0.9734\n",
            "Epoch 187/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0881 - accuracy: 0.9742 - val_loss: 0.0784 - val_accuracy: 0.9734\n",
            "Epoch 188/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0807 - accuracy: 0.9653 - val_loss: 0.0781 - val_accuracy: 0.9734\n",
            "Epoch 189/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0776 - accuracy: 0.9763 - val_loss: 0.0781 - val_accuracy: 0.9734\n",
            "Epoch 190/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0784 - accuracy: 0.9703 - val_loss: 0.0781 - val_accuracy: 0.9734\n",
            "Epoch 191/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1352 - accuracy: 0.9515 - val_loss: 0.0782 - val_accuracy: 0.9734\n",
            "Epoch 192/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1460 - accuracy: 0.9685 - val_loss: 0.0777 - val_accuracy: 0.9734\n",
            "Epoch 193/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0884 - accuracy: 0.9622 - val_loss: 0.0775 - val_accuracy: 0.9734\n",
            "Epoch 194/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0792 - accuracy: 0.9778 - val_loss: 0.0773 - val_accuracy: 0.9734\n",
            "Epoch 195/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1197 - accuracy: 0.9534 - val_loss: 0.0774 - val_accuracy: 0.9734\n",
            "Epoch 196/200\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0953 - accuracy: 0.9727 - val_loss: 0.0772 - val_accuracy: 0.9734\n",
            "Epoch 197/200\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.1004 - accuracy: 0.9602 - val_loss: 0.0777 - val_accuracy: 0.9734\n",
            "Epoch 198/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1309 - accuracy: 0.9646 - val_loss: 0.0776 - val_accuracy: 0.9734\n",
            "Epoch 199/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0770 - accuracy: 0.9737 - val_loss: 0.0776 - val_accuracy: 0.9734\n",
            "Epoch 200/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0853 - accuracy: 0.9625 - val_loss: 0.0778 - val_accuracy: 0.9734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9015c64750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA3gQKknUa_r",
        "outputId": "a0e7c0d8-5c5b-436a-8731-aaf7fe0e4d1b"
      },
      "source": [
        "print(classification_report(y_test,classifier.predict_classes(X_test)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97       123\n",
            "           1       0.97      0.92      0.94        65\n",
            "\n",
            "    accuracy                           0.96       188\n",
            "   macro avg       0.96      0.95      0.96       188\n",
            "weighted avg       0.96      0.96      0.96       188\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m9y58QFS_lg"
      },
      "source": [
        "# Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG_57xBdKi3w"
      },
      "source": [
        "from time import time\n",
        "from sklearn.metrics import classification_report,accuracy_score"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY0Q5rUKHjUD"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3h1Lo3AS6Wy",
        "outputId": "4ecfdca1-1c29-482e-eec5-34f812a4f7e8"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()\n",
        "t = time()\n",
        "logreg.fit(X_train,y_train)\n",
        "accuracy = accuracy_score(y_test, logreg.predict(X_test)) \n",
        "print(\"The accuracy of testing data: \",accuracy)\n",
        "print(\"The running time: \",time()-t)\n",
        "print('\\n')\n",
        "print('Classification Report :-')\n",
        "print(classification_report(y_test,logreg.predict(X_test)))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of testing data:  0.973404255319149\n",
            "The running time:  0.01474905014038086\n",
            "\n",
            "\n",
            "Classification Report :-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98       123\n",
            "           1       0.97      0.95      0.96        65\n",
            "\n",
            "    accuracy                           0.97       188\n",
            "   macro avg       0.97      0.97      0.97       188\n",
            "weighted avg       0.97      0.97      0.97       188\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIOk9tCaHnP1"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M23_p9UCM2in",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00abde92-1647-4717-bde4-017328511cc6"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svm = SVC(kernel = 'rbf')\n",
        "t = time()\n",
        "svm.fit(X_train,y_train)\n",
        "accuracy = accuracy_score(y_test, svm.predict(X_test)) \n",
        "print(\"The accuracy of testing data: \",accuracy)\n",
        "print(\"The running time: \",time()-t)\n",
        "print('\\n')\n",
        "print('Classification Report :-')\n",
        "print(classification_report(y_test,svm.predict(X_test)))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of testing data:  0.9627659574468085\n",
            "The running time:  0.012106180191040039\n",
            "\n",
            "\n",
            "Classification Report :-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.97       123\n",
            "           1       0.95      0.94      0.95        65\n",
            "\n",
            "    accuracy                           0.96       188\n",
            "   macro avg       0.96      0.96      0.96       188\n",
            "weighted avg       0.96      0.96      0.96       188\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqSlaEA2HpGs"
      },
      "source": [
        "XGB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Lu7dURcPf02",
        "outputId": "1a4128f3-e4a4-42a4-e296-f1909a2eec00"
      },
      "source": [
        "import xgboost as xgb\n",
        "# read in data\n",
        "dtrain = xgb.DMatrix(X_train,y_train)\n",
        "dtest = xgb.DMatrix(X_test,y_test)\n",
        "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
        "# specify parameters via map\n",
        "param = {'booster': 'dart',\n",
        "         'max_depth': 7, 'learning_rate': 0.01,\n",
        "         'objective': 'binary:logistic',\n",
        "         'sample_type': 'uniform',\n",
        "         'normalize_type': 'tree',\n",
        "         'rate_drop': 0.1,\n",
        "         'eval_metric':'auc',\n",
        "         'silent': 1,\n",
        "         'skip_drop': 0.5}\n",
        "num_round = 500\n",
        "bst = xgb.train(param, dtrain, num_round,evallist)\n",
        "preds = bst.predict(dtest)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\teval-auc:0.972295\ttrain-auc:0.998895\n",
            "[1]\teval-auc:0.973233\ttrain-auc:0.998721\n",
            "[2]\teval-auc:0.973233\ttrain-auc:0.998503\n",
            "[3]\teval-auc:0.971732\ttrain-auc:0.998474\n",
            "[4]\teval-auc:0.963102\ttrain-auc:0.998503\n",
            "[5]\teval-auc:0.970732\ttrain-auc:0.998488\n",
            "[6]\teval-auc:0.971857\ttrain-auc:0.996642\n",
            "[7]\teval-auc:0.970732\ttrain-auc:0.998532\n",
            "[8]\teval-auc:0.970732\ttrain-auc:0.998532\n",
            "[9]\teval-auc:0.970732\ttrain-auc:0.998735\n",
            "[10]\teval-auc:0.971482\ttrain-auc:0.998517\n",
            "[11]\teval-auc:0.971232\ttrain-auc:0.998735\n",
            "[12]\teval-auc:0.969731\ttrain-auc:0.998735\n",
            "[13]\teval-auc:0.971482\ttrain-auc:0.99891\n",
            "[14]\teval-auc:0.970732\ttrain-auc:0.99891\n",
            "[15]\teval-auc:0.971482\ttrain-auc:0.99891\n",
            "[16]\teval-auc:0.971482\ttrain-auc:0.99891\n",
            "[17]\teval-auc:0.970607\ttrain-auc:0.999084\n",
            "[18]\teval-auc:0.971357\ttrain-auc:0.999084\n",
            "[19]\teval-auc:0.970732\ttrain-auc:0.999084\n",
            "[20]\teval-auc:0.971482\ttrain-auc:0.999084\n",
            "[21]\teval-auc:0.970732\ttrain-auc:0.999084\n",
            "[22]\teval-auc:0.971982\ttrain-auc:0.999084\n",
            "[23]\teval-auc:0.970607\ttrain-auc:0.999084\n",
            "[24]\teval-auc:0.971482\ttrain-auc:0.999084\n",
            "[25]\teval-auc:0.970732\ttrain-auc:0.999084\n",
            "[26]\teval-auc:0.971482\ttrain-auc:0.999084\n",
            "[27]\teval-auc:0.971982\ttrain-auc:0.999084\n",
            "[28]\teval-auc:0.971482\ttrain-auc:0.999084\n",
            "[29]\teval-auc:0.970982\ttrain-auc:0.999171\n",
            "[30]\teval-auc:0.970982\ttrain-auc:0.999171\n",
            "[31]\teval-auc:0.970982\ttrain-auc:0.999084\n",
            "[32]\teval-auc:0.970857\ttrain-auc:0.999171\n",
            "[33]\teval-auc:0.970231\ttrain-auc:0.999171\n",
            "[34]\teval-auc:0.970231\ttrain-auc:0.999288\n",
            "[35]\teval-auc:0.970982\ttrain-auc:0.999288\n",
            "[36]\teval-auc:0.970231\ttrain-auc:0.999288\n",
            "[37]\teval-auc:0.970982\ttrain-auc:0.999317\n",
            "[38]\teval-auc:0.970231\ttrain-auc:0.999317\n",
            "[39]\teval-auc:0.970231\ttrain-auc:0.999288\n",
            "[40]\teval-auc:0.970857\ttrain-auc:0.999288\n",
            "[41]\teval-auc:0.970982\ttrain-auc:0.999346\n",
            "[42]\teval-auc:0.970231\ttrain-auc:0.999346\n",
            "[43]\teval-auc:0.969856\ttrain-auc:0.999375\n",
            "[44]\teval-auc:0.969856\ttrain-auc:0.999375\n",
            "[45]\teval-auc:0.969981\ttrain-auc:0.999375\n",
            "[46]\teval-auc:0.970607\ttrain-auc:0.999375\n",
            "[47]\teval-auc:0.970857\ttrain-auc:0.999346\n",
            "[48]\teval-auc:0.970732\ttrain-auc:0.999462\n",
            "[49]\teval-auc:0.969981\ttrain-auc:0.999462\n",
            "[50]\teval-auc:0.970732\ttrain-auc:0.999578\n",
            "[51]\teval-auc:0.969981\ttrain-auc:0.999462\n",
            "[52]\teval-auc:0.969981\ttrain-auc:0.999578\n",
            "[53]\teval-auc:0.969981\ttrain-auc:0.999578\n",
            "[54]\teval-auc:0.976485\ttrain-auc:0.999549\n",
            "[55]\teval-auc:0.97561\ttrain-auc:0.999462\n",
            "[56]\teval-auc:0.975109\ttrain-auc:0.999491\n",
            "[57]\teval-auc:0.97561\ttrain-auc:0.999491\n",
            "[58]\teval-auc:0.97611\ttrain-auc:0.999491\n",
            "[59]\teval-auc:0.975235\ttrain-auc:0.999549\n",
            "[60]\teval-auc:0.975109\ttrain-auc:0.999549\n",
            "[61]\teval-auc:0.975109\ttrain-auc:0.999549\n",
            "[62]\teval-auc:0.975735\ttrain-auc:0.999578\n",
            "[63]\teval-auc:0.974734\ttrain-auc:0.999549\n",
            "[64]\teval-auc:0.974609\ttrain-auc:0.999695\n",
            "[65]\teval-auc:0.974734\ttrain-auc:0.999695\n",
            "[66]\teval-auc:0.974734\ttrain-auc:0.99952\n",
            "[67]\teval-auc:0.97586\ttrain-auc:0.99952\n",
            "[68]\teval-auc:0.974734\ttrain-auc:0.999462\n",
            "[69]\teval-auc:0.974859\ttrain-auc:0.99952\n",
            "[70]\teval-auc:0.974859\ttrain-auc:0.999549\n",
            "[71]\teval-auc:0.975109\ttrain-auc:0.999491\n",
            "[72]\teval-auc:0.97536\ttrain-auc:0.999549\n",
            "[73]\teval-auc:0.975235\ttrain-auc:0.999549\n",
            "[74]\teval-auc:0.97536\ttrain-auc:0.999578\n",
            "[75]\teval-auc:0.97611\ttrain-auc:0.999549\n",
            "[76]\teval-auc:0.975109\ttrain-auc:0.999549\n",
            "[77]\teval-auc:0.974359\ttrain-auc:0.999549\n",
            "[78]\teval-auc:0.974734\ttrain-auc:0.999549\n",
            "[79]\teval-auc:0.975109\ttrain-auc:0.999549\n",
            "[80]\teval-auc:0.975485\ttrain-auc:0.999549\n",
            "[81]\teval-auc:0.974859\ttrain-auc:0.999578\n",
            "[82]\teval-auc:0.974359\ttrain-auc:0.999549\n",
            "[83]\teval-auc:0.975109\ttrain-auc:0.999549\n",
            "[84]\teval-auc:0.974359\ttrain-auc:0.999549\n",
            "[85]\teval-auc:0.975109\ttrain-auc:0.999549\n",
            "[86]\teval-auc:0.975485\ttrain-auc:0.999578\n",
            "[87]\teval-auc:0.974234\ttrain-auc:0.999549\n",
            "[88]\teval-auc:0.974234\ttrain-auc:0.999549\n",
            "[89]\teval-auc:0.974234\ttrain-auc:0.999549\n",
            "[90]\teval-auc:0.974359\ttrain-auc:0.999549\n",
            "[91]\teval-auc:0.974734\ttrain-auc:0.999549\n",
            "[92]\teval-auc:0.974734\ttrain-auc:0.999549\n",
            "[93]\teval-auc:0.974734\ttrain-auc:0.999549\n",
            "[94]\teval-auc:0.974484\ttrain-auc:0.999549\n",
            "[95]\teval-auc:0.974484\ttrain-auc:0.999549\n",
            "[96]\teval-auc:0.974234\ttrain-auc:0.999564\n",
            "[97]\teval-auc:0.974609\ttrain-auc:0.999564\n",
            "[98]\teval-auc:0.973734\ttrain-auc:0.999535\n",
            "[99]\teval-auc:0.974484\ttrain-auc:0.999564\n",
            "[100]\teval-auc:0.974484\ttrain-auc:0.999535\n",
            "[101]\teval-auc:0.973608\ttrain-auc:0.999535\n",
            "[102]\teval-auc:0.974109\ttrain-auc:0.999564\n",
            "[103]\teval-auc:0.974609\ttrain-auc:0.999535\n",
            "[104]\teval-auc:0.974609\ttrain-auc:0.999535\n",
            "[105]\teval-auc:0.974609\ttrain-auc:0.999564\n",
            "[106]\teval-auc:0.974609\ttrain-auc:0.999564\n",
            "[107]\teval-auc:0.974484\ttrain-auc:0.999564\n",
            "[108]\teval-auc:0.973984\ttrain-auc:0.999564\n",
            "[109]\teval-auc:0.973984\ttrain-auc:0.999535\n",
            "[110]\teval-auc:0.974734\ttrain-auc:0.999535\n",
            "[111]\teval-auc:0.974734\ttrain-auc:0.999535\n",
            "[112]\teval-auc:0.974359\ttrain-auc:0.999622\n",
            "[113]\teval-auc:0.974234\ttrain-auc:0.999535\n",
            "[114]\teval-auc:0.974234\ttrain-auc:0.999535\n",
            "[115]\teval-auc:0.973859\ttrain-auc:0.999535\n",
            "[116]\teval-auc:0.974234\ttrain-auc:0.999535\n",
            "[117]\teval-auc:0.973734\ttrain-auc:0.999535\n",
            "[118]\teval-auc:0.974234\ttrain-auc:0.999535\n",
            "[119]\teval-auc:0.974359\ttrain-auc:0.999535\n",
            "[120]\teval-auc:0.974296\ttrain-auc:0.999535\n",
            "[121]\teval-auc:0.973796\ttrain-auc:0.999535\n",
            "[122]\teval-auc:0.974296\ttrain-auc:0.999535\n",
            "[123]\teval-auc:0.972983\ttrain-auc:0.999535\n",
            "[124]\teval-auc:0.973671\ttrain-auc:0.999622\n",
            "[125]\teval-auc:0.974296\ttrain-auc:0.999535\n",
            "[126]\teval-auc:0.973921\ttrain-auc:0.999535\n",
            "[127]\teval-auc:0.974171\ttrain-auc:0.999564\n",
            "[128]\teval-auc:0.974171\ttrain-auc:0.999651\n",
            "[129]\teval-auc:0.974171\ttrain-auc:0.999564\n",
            "[130]\teval-auc:0.974171\ttrain-auc:0.999651\n",
            "[131]\teval-auc:0.974171\ttrain-auc:0.999622\n",
            "[132]\teval-auc:0.973546\ttrain-auc:0.999564\n",
            "[133]\teval-auc:0.974171\ttrain-auc:0.999593\n",
            "[134]\teval-auc:0.972545\ttrain-auc:0.999651\n",
            "[135]\teval-auc:0.974046\ttrain-auc:0.999564\n",
            "[136]\teval-auc:0.973921\ttrain-auc:0.999651\n",
            "[137]\teval-auc:0.973921\ttrain-auc:0.999709\n",
            "[138]\teval-auc:0.973546\ttrain-auc:0.999651\n",
            "[139]\teval-auc:0.973546\ttrain-auc:0.99968\n",
            "[140]\teval-auc:0.973421\ttrain-auc:0.999709\n",
            "[141]\teval-auc:0.974171\ttrain-auc:0.99968\n",
            "[142]\teval-auc:0.974171\ttrain-auc:0.999622\n",
            "[143]\teval-auc:0.974547\ttrain-auc:0.99968\n",
            "[144]\teval-auc:0.973921\ttrain-auc:0.999709\n",
            "[145]\teval-auc:0.973671\ttrain-auc:0.999767\n",
            "[146]\teval-auc:0.973921\ttrain-auc:0.999709\n",
            "[147]\teval-auc:0.974171\ttrain-auc:0.999709\n",
            "[148]\teval-auc:0.974046\ttrain-auc:0.999767\n",
            "[149]\teval-auc:0.973546\ttrain-auc:0.999767\n",
            "[150]\teval-auc:0.973671\ttrain-auc:0.999767\n",
            "[151]\teval-auc:0.974046\ttrain-auc:0.999767\n",
            "[152]\teval-auc:0.974046\ttrain-auc:0.999738\n",
            "[153]\teval-auc:0.974109\ttrain-auc:0.999738\n",
            "[154]\teval-auc:0.974234\ttrain-auc:0.999767\n",
            "[155]\teval-auc:0.974234\ttrain-auc:0.999767\n",
            "[156]\teval-auc:0.974046\ttrain-auc:0.999738\n",
            "[157]\teval-auc:0.974234\ttrain-auc:0.999767\n",
            "[158]\teval-auc:0.974734\ttrain-auc:0.999767\n",
            "[159]\teval-auc:0.974859\ttrain-auc:0.999738\n",
            "[160]\teval-auc:0.973608\ttrain-auc:0.999767\n",
            "[161]\teval-auc:0.974984\ttrain-auc:0.999767\n",
            "[162]\teval-auc:0.974859\ttrain-auc:0.999767\n",
            "[163]\teval-auc:0.974859\ttrain-auc:0.999767\n",
            "[164]\teval-auc:0.974984\ttrain-auc:0.999767\n",
            "[165]\teval-auc:0.975235\ttrain-auc:0.999738\n",
            "[166]\teval-auc:0.974734\ttrain-auc:0.999767\n",
            "[167]\teval-auc:0.975109\ttrain-auc:0.999767\n",
            "[168]\teval-auc:0.974984\ttrain-auc:0.999767\n",
            "[169]\teval-auc:0.974984\ttrain-auc:0.999767\n",
            "[170]\teval-auc:0.975485\ttrain-auc:0.999738\n",
            "[171]\teval-auc:0.97561\ttrain-auc:0.999767\n",
            "[172]\teval-auc:0.975109\ttrain-auc:0.999767\n",
            "[173]\teval-auc:0.974484\ttrain-auc:0.999767\n",
            "[174]\teval-auc:0.975109\ttrain-auc:0.999767\n",
            "[175]\teval-auc:0.97561\ttrain-auc:0.999767\n",
            "[176]\teval-auc:0.974984\ttrain-auc:0.999767\n",
            "[177]\teval-auc:0.975109\ttrain-auc:0.999767\n",
            "[178]\teval-auc:0.975235\ttrain-auc:0.999767\n",
            "[179]\teval-auc:0.97536\ttrain-auc:0.999767\n",
            "[180]\teval-auc:0.97561\ttrain-auc:0.999767\n",
            "[181]\teval-auc:0.975735\ttrain-auc:0.999797\n",
            "[182]\teval-auc:0.975985\ttrain-auc:0.999797\n",
            "[183]\teval-auc:0.975735\ttrain-auc:0.999826\n",
            "[184]\teval-auc:0.975735\ttrain-auc:0.999826\n",
            "[185]\teval-auc:0.97561\ttrain-auc:0.999797\n",
            "[186]\teval-auc:0.97586\ttrain-auc:0.999826\n",
            "[187]\teval-auc:0.97586\ttrain-auc:0.999826\n",
            "[188]\teval-auc:0.97586\ttrain-auc:0.999826\n",
            "[189]\teval-auc:0.97586\ttrain-auc:0.999855\n",
            "[190]\teval-auc:0.97586\ttrain-auc:0.999826\n",
            "[191]\teval-auc:0.97561\ttrain-auc:0.999855\n",
            "[192]\teval-auc:0.97586\ttrain-auc:0.999855\n",
            "[193]\teval-auc:0.97586\ttrain-auc:0.999855\n",
            "[194]\teval-auc:0.97611\ttrain-auc:0.999826\n",
            "[195]\teval-auc:0.97586\ttrain-auc:0.999855\n",
            "[196]\teval-auc:0.976485\ttrain-auc:0.999855\n",
            "[197]\teval-auc:0.97661\ttrain-auc:0.999855\n",
            "[198]\teval-auc:0.97586\ttrain-auc:0.999855\n",
            "[199]\teval-auc:0.97586\ttrain-auc:0.999797\n",
            "[200]\teval-auc:0.97586\ttrain-auc:0.999826\n",
            "[201]\teval-auc:0.976235\ttrain-auc:0.999797\n",
            "[202]\teval-auc:0.976235\ttrain-auc:0.999884\n",
            "[203]\teval-auc:0.976173\ttrain-auc:0.999855\n",
            "[204]\teval-auc:0.975797\ttrain-auc:0.999855\n",
            "[205]\teval-auc:0.976235\ttrain-auc:0.999855\n",
            "[206]\teval-auc:0.976798\ttrain-auc:0.999855\n",
            "[207]\teval-auc:0.976735\ttrain-auc:0.999855\n",
            "[208]\teval-auc:0.976673\ttrain-auc:0.999855\n",
            "[209]\teval-auc:0.976673\ttrain-auc:0.999884\n",
            "[210]\teval-auc:0.976673\ttrain-auc:0.999913\n",
            "[211]\teval-auc:0.976673\ttrain-auc:0.999913\n",
            "[212]\teval-auc:0.976673\ttrain-auc:0.999826\n",
            "[213]\teval-auc:0.976673\ttrain-auc:0.999913\n",
            "[214]\teval-auc:0.976673\ttrain-auc:0.999913\n",
            "[215]\teval-auc:0.977298\ttrain-auc:0.999913\n",
            "[216]\teval-auc:0.976673\ttrain-auc:0.999913\n",
            "[217]\teval-auc:0.977173\ttrain-auc:0.999913\n",
            "[218]\teval-auc:0.976673\ttrain-auc:0.999913\n",
            "[219]\teval-auc:0.977173\ttrain-auc:0.999884\n",
            "[220]\teval-auc:0.977548\ttrain-auc:0.999913\n",
            "[221]\teval-auc:0.976298\ttrain-auc:0.999913\n",
            "[222]\teval-auc:0.977548\ttrain-auc:0.999884\n",
            "[223]\teval-auc:0.977548\ttrain-auc:0.999884\n",
            "[224]\teval-auc:0.977548\ttrain-auc:0.999884\n",
            "[225]\teval-auc:0.977298\ttrain-auc:0.999913\n",
            "[226]\teval-auc:0.977548\ttrain-auc:0.999884\n",
            "[227]\teval-auc:0.977799\ttrain-auc:0.999884\n",
            "[228]\teval-auc:0.977799\ttrain-auc:0.999884\n",
            "[229]\teval-auc:0.977799\ttrain-auc:0.999913\n",
            "[230]\teval-auc:0.977173\ttrain-auc:0.999884\n",
            "[231]\teval-auc:0.977924\ttrain-auc:0.999884\n",
            "[232]\teval-auc:0.978299\ttrain-auc:0.999884\n",
            "[233]\teval-auc:0.977924\ttrain-auc:0.999884\n",
            "[234]\teval-auc:0.977924\ttrain-auc:0.999884\n",
            "[235]\teval-auc:0.977924\ttrain-auc:0.999913\n",
            "[236]\teval-auc:0.977924\ttrain-auc:0.999913\n",
            "[237]\teval-auc:0.978299\ttrain-auc:0.999855\n",
            "[238]\teval-auc:0.978299\ttrain-auc:0.999884\n",
            "[239]\teval-auc:0.977674\ttrain-auc:0.999913\n",
            "[240]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[241]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[242]\teval-auc:0.978424\ttrain-auc:0.999913\n",
            "[243]\teval-auc:0.977924\ttrain-auc:0.999913\n",
            "[244]\teval-auc:0.978424\ttrain-auc:0.999913\n",
            "[245]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[246]\teval-auc:0.977799\ttrain-auc:0.999913\n",
            "[247]\teval-auc:0.978299\ttrain-auc:0.999884\n",
            "[248]\teval-auc:0.978299\ttrain-auc:0.999884\n",
            "[249]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[250]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[251]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[252]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[253]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[254]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[255]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[256]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[257]\teval-auc:0.978299\ttrain-auc:0.999884\n",
            "[258]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[259]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[260]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[261]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[262]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[263]\teval-auc:0.978299\ttrain-auc:0.999884\n",
            "[264]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[265]\teval-auc:0.978424\ttrain-auc:0.999884\n",
            "[266]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[267]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[268]\teval-auc:0.978299\ttrain-auc:0.999884\n",
            "[269]\teval-auc:0.978174\ttrain-auc:0.999884\n",
            "[270]\teval-auc:0.978174\ttrain-auc:0.999913\n",
            "[271]\teval-auc:0.978174\ttrain-auc:0.999884\n",
            "[272]\teval-auc:0.978174\ttrain-auc:0.999884\n",
            "[273]\teval-auc:0.978174\ttrain-auc:0.999913\n",
            "[274]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[275]\teval-auc:0.977924\ttrain-auc:0.999913\n",
            "[276]\teval-auc:0.977924\ttrain-auc:0.999913\n",
            "[277]\teval-auc:0.978174\ttrain-auc:0.999913\n",
            "[278]\teval-auc:0.978174\ttrain-auc:0.999884\n",
            "[279]\teval-auc:0.978049\ttrain-auc:0.999913\n",
            "[280]\teval-auc:0.978236\ttrain-auc:0.999913\n",
            "[281]\teval-auc:0.978236\ttrain-auc:0.999913\n",
            "[282]\teval-auc:0.978612\ttrain-auc:0.999884\n",
            "[283]\teval-auc:0.978361\ttrain-auc:0.999884\n",
            "[284]\teval-auc:0.978361\ttrain-auc:0.999913\n",
            "[285]\teval-auc:0.978361\ttrain-auc:0.999913\n",
            "[286]\teval-auc:0.978236\ttrain-auc:0.999913\n",
            "[287]\teval-auc:0.978236\ttrain-auc:0.999913\n",
            "[288]\teval-auc:0.978612\ttrain-auc:0.999884\n",
            "[289]\teval-auc:0.977986\ttrain-auc:0.999884\n",
            "[290]\teval-auc:0.978487\ttrain-auc:0.999884\n",
            "[291]\teval-auc:0.978487\ttrain-auc:0.999884\n",
            "[292]\teval-auc:0.978612\ttrain-auc:0.999913\n",
            "[293]\teval-auc:0.978612\ttrain-auc:0.999913\n",
            "[294]\teval-auc:0.977861\ttrain-auc:0.999913\n",
            "[295]\teval-auc:0.978487\ttrain-auc:0.999913\n",
            "[296]\teval-auc:0.978612\ttrain-auc:0.999884\n",
            "[297]\teval-auc:0.978737\ttrain-auc:0.999913\n",
            "[298]\teval-auc:0.978612\ttrain-auc:0.999913\n",
            "[299]\teval-auc:0.978737\ttrain-auc:0.999913\n",
            "[300]\teval-auc:0.978737\ttrain-auc:0.999913\n",
            "[301]\teval-auc:0.978612\ttrain-auc:0.999913\n",
            "[302]\teval-auc:0.978862\ttrain-auc:0.999913\n",
            "[303]\teval-auc:0.978737\ttrain-auc:0.999913\n",
            "[304]\teval-auc:0.978862\ttrain-auc:0.999913\n",
            "[305]\teval-auc:0.978737\ttrain-auc:0.999913\n",
            "[306]\teval-auc:0.978862\ttrain-auc:0.999884\n",
            "[307]\teval-auc:0.978674\ttrain-auc:0.999884\n",
            "[308]\teval-auc:0.978487\ttrain-auc:0.999913\n",
            "[309]\teval-auc:0.978487\ttrain-auc:0.999913\n",
            "[310]\teval-auc:0.978361\ttrain-auc:0.999913\n",
            "[311]\teval-auc:0.978487\ttrain-auc:0.999913\n",
            "[312]\teval-auc:0.978236\ttrain-auc:0.999913\n",
            "[313]\teval-auc:0.978737\ttrain-auc:0.999913\n",
            "[314]\teval-auc:0.978487\ttrain-auc:0.999913\n",
            "[315]\teval-auc:0.978487\ttrain-auc:0.999913\n",
            "[316]\teval-auc:0.978487\ttrain-auc:0.999913\n",
            "[317]\teval-auc:0.978487\ttrain-auc:0.999913\n",
            "[318]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[319]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[320]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[321]\teval-auc:0.978549\ttrain-auc:0.999913\n",
            "[322]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[323]\teval-auc:0.979049\ttrain-auc:0.999913\n",
            "[324]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[325]\teval-auc:0.978549\ttrain-auc:0.999913\n",
            "[326]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[327]\teval-auc:0.978674\ttrain-auc:0.999913\n",
            "[328]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[329]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[330]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[331]\teval-auc:0.978174\ttrain-auc:0.999913\n",
            "[332]\teval-auc:0.978862\ttrain-auc:0.999884\n",
            "[333]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[334]\teval-auc:0.978549\ttrain-auc:0.999913\n",
            "[335]\teval-auc:0.978674\ttrain-auc:0.999913\n",
            "[336]\teval-auc:0.978299\ttrain-auc:0.999913\n",
            "[337]\teval-auc:0.978549\ttrain-auc:0.999913\n",
            "[338]\teval-auc:0.978549\ttrain-auc:0.999913\n",
            "[339]\teval-auc:0.978424\ttrain-auc:0.999913\n",
            "[340]\teval-auc:0.978174\ttrain-auc:0.999942\n",
            "[341]\teval-auc:0.978424\ttrain-auc:0.999913\n",
            "[342]\teval-auc:0.978174\ttrain-auc:0.999942\n",
            "[343]\teval-auc:0.978487\ttrain-auc:0.999913\n",
            "[344]\teval-auc:0.978424\ttrain-auc:0.999913\n",
            "[345]\teval-auc:0.978424\ttrain-auc:0.999913\n",
            "[346]\teval-auc:0.978674\ttrain-auc:0.999913\n",
            "[347]\teval-auc:0.978424\ttrain-auc:0.999913\n",
            "[348]\teval-auc:0.978424\ttrain-auc:0.999884\n",
            "[349]\teval-auc:0.978424\ttrain-auc:0.999942\n",
            "[350]\teval-auc:0.978424\ttrain-auc:0.999942\n",
            "[351]\teval-auc:0.978424\ttrain-auc:0.999884\n",
            "[352]\teval-auc:0.978424\ttrain-auc:0.999942\n",
            "[353]\teval-auc:0.978424\ttrain-auc:0.999942\n",
            "[354]\teval-auc:0.978424\ttrain-auc:0.999942\n",
            "[355]\teval-auc:0.978424\ttrain-auc:0.999942\n",
            "[356]\teval-auc:0.978424\ttrain-auc:0.999942\n",
            "[357]\teval-auc:0.978424\ttrain-auc:0.999942\n",
            "[358]\teval-auc:0.978674\ttrain-auc:0.999942\n",
            "[359]\teval-auc:0.978424\ttrain-auc:0.999942\n",
            "[360]\teval-auc:0.978424\ttrain-auc:0.999942\n",
            "[361]\teval-auc:0.978424\ttrain-auc:0.999942\n",
            "[362]\teval-auc:0.978174\ttrain-auc:0.999942\n",
            "[363]\teval-auc:0.978424\ttrain-auc:0.999942\n",
            "[364]\teval-auc:0.978424\ttrain-auc:0.999942\n",
            "[365]\teval-auc:0.978424\ttrain-auc:0.999942\n",
            "[366]\teval-auc:0.978174\ttrain-auc:0.999942\n",
            "[367]\teval-auc:0.978549\ttrain-auc:0.999942\n",
            "[368]\teval-auc:0.978549\ttrain-auc:0.999913\n",
            "[369]\teval-auc:0.979174\ttrain-auc:0.999942\n",
            "[370]\teval-auc:0.978361\ttrain-auc:0.999942\n",
            "[371]\teval-auc:0.978361\ttrain-auc:0.999942\n",
            "[372]\teval-auc:0.978361\ttrain-auc:0.999913\n",
            "[373]\teval-auc:0.978361\ttrain-auc:0.999942\n",
            "[374]\teval-auc:0.978236\ttrain-auc:0.999913\n",
            "[375]\teval-auc:0.978361\ttrain-auc:0.999913\n",
            "[376]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[377]\teval-auc:0.978862\ttrain-auc:0.999942\n",
            "[378]\teval-auc:0.978361\ttrain-auc:0.999913\n",
            "[379]\teval-auc:0.978361\ttrain-auc:0.999942\n",
            "[380]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[381]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[382]\teval-auc:0.978737\ttrain-auc:0.999942\n",
            "[383]\teval-auc:0.978361\ttrain-auc:0.999942\n",
            "[384]\teval-auc:0.978361\ttrain-auc:0.999942\n",
            "[385]\teval-auc:0.978361\ttrain-auc:0.999942\n",
            "[386]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[387]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[388]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[389]\teval-auc:0.978737\ttrain-auc:0.999942\n",
            "[390]\teval-auc:0.978361\ttrain-auc:0.999942\n",
            "[391]\teval-auc:0.978487\ttrain-auc:0.999913\n",
            "[392]\teval-auc:0.978361\ttrain-auc:0.999942\n",
            "[393]\teval-auc:0.978361\ttrain-auc:0.999942\n",
            "[394]\teval-auc:0.978361\ttrain-auc:0.999942\n",
            "[395]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[396]\teval-auc:0.977986\ttrain-auc:0.999942\n",
            "[397]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[398]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[399]\teval-auc:0.978737\ttrain-auc:0.999942\n",
            "[400]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[401]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[402]\teval-auc:0.978487\ttrain-auc:0.999913\n",
            "[403]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[404]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[405]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[406]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[407]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[408]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[409]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[410]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[411]\teval-auc:0.978424\ttrain-auc:0.999942\n",
            "[412]\teval-auc:0.978361\ttrain-auc:0.999942\n",
            "[413]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[414]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[415]\teval-auc:0.978361\ttrain-auc:0.999942\n",
            "[416]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[417]\teval-auc:0.978737\ttrain-auc:0.999942\n",
            "[418]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[419]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[420]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[421]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[422]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[423]\teval-auc:0.978236\ttrain-auc:0.999942\n",
            "[424]\teval-auc:0.978737\ttrain-auc:0.999942\n",
            "[425]\teval-auc:0.978737\ttrain-auc:0.999942\n",
            "[426]\teval-auc:0.978361\ttrain-auc:0.999942\n",
            "[427]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[428]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[429]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[430]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[431]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[432]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[433]\teval-auc:0.978111\ttrain-auc:0.999942\n",
            "[434]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[435]\teval-auc:0.978174\ttrain-auc:0.999942\n",
            "[436]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[437]\teval-auc:0.978737\ttrain-auc:0.999942\n",
            "[438]\teval-auc:0.978612\ttrain-auc:0.999913\n",
            "[439]\teval-auc:0.978862\ttrain-auc:0.999942\n",
            "[440]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[441]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[442]\teval-auc:0.978737\ttrain-auc:0.999942\n",
            "[443]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[444]\teval-auc:0.978612\ttrain-auc:0.999913\n",
            "[445]\teval-auc:0.978737\ttrain-auc:0.999913\n",
            "[446]\teval-auc:0.978737\ttrain-auc:0.999942\n",
            "[447]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[448]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[449]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[450]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[451]\teval-auc:0.978487\ttrain-auc:0.999913\n",
            "[452]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[453]\teval-auc:0.978612\ttrain-auc:0.999913\n",
            "[454]\teval-auc:0.978737\ttrain-auc:0.999942\n",
            "[455]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[456]\teval-auc:0.978737\ttrain-auc:0.999942\n",
            "[457]\teval-auc:0.978361\ttrain-auc:0.999942\n",
            "[458]\teval-auc:0.978737\ttrain-auc:0.999942\n",
            "[459]\teval-auc:0.978737\ttrain-auc:0.999942\n",
            "[460]\teval-auc:0.978737\ttrain-auc:0.999942\n",
            "[461]\teval-auc:0.978487\ttrain-auc:0.999942\n",
            "[462]\teval-auc:0.978612\ttrain-auc:1\n",
            "[463]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[464]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[465]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[466]\teval-auc:0.978487\ttrain-auc:1\n",
            "[467]\teval-auc:0.978737\ttrain-auc:0.999971\n",
            "[468]\teval-auc:0.978737\ttrain-auc:0.999971\n",
            "[469]\teval-auc:0.978737\ttrain-auc:0.999942\n",
            "[470]\teval-auc:0.978487\ttrain-auc:0.999971\n",
            "[471]\teval-auc:0.978737\ttrain-auc:0.999971\n",
            "[472]\teval-auc:0.978487\ttrain-auc:0.999971\n",
            "[473]\teval-auc:0.978487\ttrain-auc:0.999971\n",
            "[474]\teval-auc:0.978862\ttrain-auc:1\n",
            "[475]\teval-auc:0.978487\ttrain-auc:1\n",
            "[476]\teval-auc:0.978737\ttrain-auc:1\n",
            "[477]\teval-auc:0.978612\ttrain-auc:1\n",
            "[478]\teval-auc:0.978612\ttrain-auc:1\n",
            "[479]\teval-auc:0.978612\ttrain-auc:1\n",
            "[480]\teval-auc:0.978612\ttrain-auc:1\n",
            "[481]\teval-auc:0.978612\ttrain-auc:1\n",
            "[482]\teval-auc:0.978361\ttrain-auc:0.999971\n",
            "[483]\teval-auc:0.978612\ttrain-auc:0.999942\n",
            "[484]\teval-auc:0.978612\ttrain-auc:0.999971\n",
            "[485]\teval-auc:0.978487\ttrain-auc:1\n",
            "[486]\teval-auc:0.982239\ttrain-auc:1\n",
            "[487]\teval-auc:0.982114\ttrain-auc:1\n",
            "[488]\teval-auc:0.982114\ttrain-auc:1\n",
            "[489]\teval-auc:0.982364\ttrain-auc:1\n",
            "[490]\teval-auc:0.982239\ttrain-auc:0.999971\n",
            "[491]\teval-auc:0.982489\ttrain-auc:1\n",
            "[492]\teval-auc:0.982114\ttrain-auc:1\n",
            "[493]\teval-auc:0.982364\ttrain-auc:0.999971\n",
            "[494]\teval-auc:0.982114\ttrain-auc:1\n",
            "[495]\teval-auc:0.981989\ttrain-auc:0.999942\n",
            "[496]\teval-auc:0.982114\ttrain-auc:1\n",
            "[497]\teval-auc:0.981989\ttrain-auc:1\n",
            "[498]\teval-auc:0.981989\ttrain-auc:1\n",
            "[499]\teval-auc:0.981989\ttrain-auc:0.999971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "jvYfXvRqQHlA",
        "outputId": "06a7a7eb-a921-4a97-db69-5b64a4598899"
      },
      "source": [
        "xgb.plot_importance(bst)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0c59ffe510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWESi8w4CoQi"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeYEjVMx86R5",
        "outputId": "b316fe1e-972f-401b-ae5e-6e5e69991b5a"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc =RandomForestClassifier(n_estimators=100)\n",
        "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
        "param_grid = { \n",
        "    'n_estimators': [200, 100],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth' : [4,5,6,7,8],\n",
        "    'criterion' :['gini', 'entropy']\n",
        "}\n",
        "CV_rfc = GridSearchCV(estimator= rfc, \n",
        "                          param_grid= param_grid,\n",
        "                          cv=StratifiedKFold(), \n",
        "                          n_jobs=-1, \n",
        "                          scoring='accuracy', \n",
        "                          verbose=2).fit(X_train, y_train)\n",
        "CV_rfc.best_params_"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:    9.6s\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:   35.8s\n",
            "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'criterion': 'entropy',\n",
              " 'max_depth': 7,\n",
              " 'max_features': 'auto',\n",
              " 'n_estimators': 100}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0_HyRxHC4vR",
        "outputId": "2d9d4d84-dc83-4b09-c898-53b4a3f3dff4"
      },
      "source": [
        "rfc = RandomForestClassifier(**CV_rfc.best_params_)\n",
        "rfc.fit(X_train,y_train)\n",
        "t = time()\n",
        "accuracy = accuracy_score(y_test, rfc.predict(X_test)) \n",
        "print(\"The accuracy of testing data: \",accuracy)\n",
        "print(\"The running time: \",time()-t)\n",
        "print('\\n')\n",
        "print('Classification Report :-')\n",
        "print(classification_report(y_test,rfc.predict(X_test)))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of testing data:  0.9468085106382979\n",
            "The running time:  0.010782957077026367\n",
            "\n",
            "\n",
            "Classification Report :-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.96      0.96       123\n",
            "           1       0.92      0.92      0.92        65\n",
            "\n",
            "    accuracy                           0.95       188\n",
            "   macro avg       0.94      0.94      0.94       188\n",
            "weighted avg       0.95      0.95      0.95       188\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Baxp97L9HJ"
      },
      "source": [
        " Adaboost Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ednGFFZgMATn",
        "outputId": "81695709-ba95-4b59-e445-65429757d75d"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "adc = AdaBoostClassifier()\n",
        "t = time()\n",
        "adc.fit(X_train,y_train)\n",
        "accuracy = accuracy_score(y_test, adc.predict(X_test)) \n",
        "print(\"The accuracy of testing data: \",accuracy)\n",
        "print(\"The running time: \",time()-t)\n",
        "print('\\n')\n",
        "print('Classification Report :-')\n",
        "print(classification_report(y_test,adc.predict(X_test)))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of testing data:  0.9521276595744681\n",
            "The running time:  0.14768576622009277\n",
            "\n",
            "\n",
            "Classification Report :-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96       123\n",
            "           1       0.94      0.92      0.93        65\n",
            "\n",
            "    accuracy                           0.95       188\n",
            "   macro avg       0.95      0.95      0.95       188\n",
            "weighted avg       0.95      0.95      0.95       188\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clXGC_zqMob8"
      },
      "source": [
        "Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DrjDvREMlwF",
        "outputId": "0ef707b5-b403-4eca-f1d5-c6a63e01f497"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "cart = DecisionTreeClassifier()\n",
        "t = time()\n",
        "cart.fit(X_train,y_train)\n",
        "accuracy = accuracy_score(y_test, cart.predict(X_test)) \n",
        "print(\"The accuracy of testing data: \",accuracy)\n",
        "print(\"The running time: \",time()-t)\n",
        "print('\\n')\n",
        "print('Classification Report :-')\n",
        "print(classification_report(y_test,cart.predict(X_test)))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of testing data:  0.9414893617021277\n",
            "The running time:  0.010379552841186523\n",
            "\n",
            "\n",
            "Classification Report :-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96       123\n",
            "           1       0.95      0.88      0.91        65\n",
            "\n",
            "    accuracy                           0.94       188\n",
            "   macro avg       0.94      0.93      0.93       188\n",
            "weighted avg       0.94      0.94      0.94       188\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FonsBG7NewZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1ihJCjcNkHB"
      },
      "source": [
        "# Stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vN1GEGkNmv8",
        "outputId": "edebdcf2-6244-42e1-d119-97526f09166d"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "t = time()\n",
        "vc = VotingClassifier([('logreg',logreg),('svm',svm),('rf',rfc)])\n",
        "vc.fit(X_train,y_train)\n",
        "accuracy = accuracy_score(y_test, vc.predict(X_test)) \n",
        "print(\"The accuracy of testing data: \",accuracy)\n",
        "print(\"The running time: \",time()-t)\n",
        "print('\\n')\n",
        "print('Classification Report :-')\n",
        "print(classification_report(y_test,vc.predict(X_test)))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of testing data:  0.973404255319149\n",
            "The running time:  0.30020904541015625\n",
            "\n",
            "\n",
            "Classification Report :-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98       123\n",
            "           1       0.97      0.95      0.96        65\n",
            "\n",
            "    accuracy                           0.97       188\n",
            "   macro avg       0.97      0.97      0.97       188\n",
            "weighted avg       0.97      0.97      0.97       188\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}